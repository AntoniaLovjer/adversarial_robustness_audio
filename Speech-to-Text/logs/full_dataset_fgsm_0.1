[epoch: 1, batch:     20] loss: 0.08909 time model: 0.03959 acc: 0.05625
[epoch: 1, batch:     40] loss: 0.08177 time model: 0.03952 acc: 0.08047
[epoch: 1, batch:     60] loss: 0.07868 time model: 0.03947 acc: 0.09167
[epoch: 1, batch:     80] loss: 0.07673 time model: 0.03947 acc: 0.10039
[epoch: 1, batch:    100] loss: 0.07559 time model: 0.03946 acc: 0.11156
[epoch: 1, batch:    120] loss: 0.07441 time model: 0.03946 acc: 0.12526
[epoch: 1, batch:    140] loss: 0.07358 time model: 0.03947 acc: 0.13594
[epoch: 1, batch:    160] loss: 0.07327 time model: 0.03948 acc: 0.14141
[epoch: 1, batch:    180] loss: 0.07258 time model: 0.03950 acc: 0.14896
[epoch: 1, batch:    200] loss: 0.07183 time model: 0.03950 acc: 0.15594
[epoch: 1, batch:    220] loss: 0.07162 time model: 0.03950 acc: 0.15653
[epoch: 1, batch:    240] loss: 0.07113 time model: 0.03950 acc: 0.16172
[epoch: 1, batch:    260] loss: 0.07061 time model: 0.03950 acc: 0.16815
[epoch: 1, batch:    280] loss: 0.07014 time model: 0.03950 acc: 0.17188
[epoch: 1, batch:    300] loss: 0.07000 time model: 0.03951 acc: 0.17375
[epoch: 1, batch:    320] loss: 0.06965 time model: 0.03952 acc: 0.17705
[epoch: 1, batch:    340] loss: 0.06934 time model: 0.03952 acc: 0.18171
[epoch: 1, batch:    360] loss: 0.06893 time model: 0.03952 acc: 0.18672
[epoch: 1, batch:    380] loss: 0.06855 time model: 0.03953 acc: 0.19112
[epoch: 1, batch:    400] loss: 0.06824 time model: 0.03953 acc: 0.19555
[epoch: 1, batch:    420] loss: 0.06793 time model: 0.03953 acc: 0.19970
[epoch: 1, batch:    440] loss: 0.06750 time model: 0.03954 acc: 0.20426
[epoch: 1, batch:    460] loss: 0.06716 time model: 0.03955 acc: 0.20938
[epoch: 1, batch:    480] loss: 0.06671 time model: 0.03955 acc: 0.21322
[epoch: 1, batch:    500] loss: 0.06631 time model: 0.03955 acc: 0.21806
[epoch: 1, batch:    520] loss: 0.06588 time model: 0.03955 acc: 0.22308
[epoch: 1, batch:    540] loss: 0.06530 time model: 0.03956 acc: 0.22951
[epoch: 1, batch:    560] loss: 0.06482 time model: 0.03956 acc: 0.23516
[epoch: 1, batch:    580] loss: 0.06426 time model: 0.03956 acc: 0.24133
[epoch: 1, batch:    600] loss: 0.06377 time model: 0.03956 acc: 0.24609
[epoch: 1, batch:    620] loss: 0.06326 time model: 0.03956 acc: 0.25146
[epoch: 1, batch:    640] loss: 0.06266 time model: 0.03957 acc: 0.25796
[epoch: 1, batch:    660] loss: 0.06231 time model: 0.03958 acc: 0.26278
[epoch: 1, batch:     20] loss: 0.07215 time model: 0.02567 acc: 0.21094
[epoch: 1, batch:     40] loss: 0.07031 time model: 0.02565 acc: 0.22187
[epoch: 1, batch:     60] loss: 0.07078 time model: 0.02565 acc: 0.22500
[epoch: 1, batch:     80] loss: 0.07199 time model: 0.02563 acc: 0.22539
epoch:1 train loss: 0.06231041349977205 train acc: 0.2627813314380479 valid loss: 0.07248090818773002 valid acc: 0.22545595653861078
[epoch: 2, batch:     20] loss: 0.04581 time model: 0.03949 acc: 0.44531
[epoch: 2, batch:     40] loss: 0.04382 time model: 0.03949 acc: 0.46563
[epoch: 2, batch:     60] loss: 0.04355 time model: 0.03949 acc: 0.46823
[epoch: 2, batch:     80] loss: 0.04301 time model: 0.03948 acc: 0.47617
[epoch: 2, batch:    100] loss: 0.04254 time model: 0.03947 acc: 0.47875
[epoch: 2, batch:    120] loss: 0.04233 time model: 0.03946 acc: 0.48411
[epoch: 2, batch:    140] loss: 0.04209 time model: 0.03948 acc: 0.48728
[epoch: 2, batch:    160] loss: 0.04154 time model: 0.03949 acc: 0.49355
[epoch: 2, batch:    180] loss: 0.04150 time model: 0.03951 acc: 0.49601
[epoch: 2, batch:    200] loss: 0.04115 time model: 0.03952 acc: 0.49969
[epoch: 2, batch:    220] loss: 0.04085 time model: 0.03952 acc: 0.50327
[epoch: 2, batch:    240] loss: 0.04043 time model: 0.03952 acc: 0.50794
[epoch: 2, batch:    260] loss: 0.04029 time model: 0.03952 acc: 0.50938
[epoch: 2, batch:    280] loss: 0.04013 time model: 0.03952 acc: 0.51384
[epoch: 2, batch:    300] loss: 0.03992 time model: 0.03953 acc: 0.51677
[epoch: 2, batch:    320] loss: 0.03994 time model: 0.03953 acc: 0.51689
[epoch: 2, batch:    340] loss: 0.03961 time model: 0.03953 acc: 0.52013
[epoch: 2, batch:    360] loss: 0.03942 time model: 0.03953 acc: 0.52283
[epoch: 2, batch:    380] loss: 0.03922 time model: 0.03953 acc: 0.52558
[epoch: 2, batch:    400] loss: 0.03901 time model: 0.03952 acc: 0.52812
[epoch: 2, batch:    420] loss: 0.03890 time model: 0.03952 acc: 0.52857
[epoch: 2, batch:    440] loss: 0.03860 time model: 0.03952 acc: 0.53161
[epoch: 2, batch:    460] loss: 0.03842 time model: 0.03952 acc: 0.53410
[epoch: 2, batch:    480] loss: 0.03822 time model: 0.03951 acc: 0.53639
[epoch: 2, batch:    500] loss: 0.03801 time model: 0.03951 acc: 0.53944
[epoch: 2, batch:    520] loss: 0.03772 time model: 0.03951 acc: 0.54267
[epoch: 2, batch:    540] loss: 0.03752 time model: 0.03950 acc: 0.54473
[epoch: 2, batch:    560] loss: 0.03734 time model: 0.03950 acc: 0.54771
[epoch: 2, batch:    580] loss: 0.03715 time model: 0.03950 acc: 0.55027
[epoch: 2, batch:    600] loss: 0.03688 time model: 0.03950 acc: 0.55318
[epoch: 2, batch:    620] loss: 0.03664 time model: 0.03950 acc: 0.55590
[epoch: 2, batch:    640] loss: 0.03646 time model: 0.03950 acc: 0.55845
[epoch: 2, batch:    660] loss: 0.03626 time model: 0.03951 acc: 0.56148
[epoch: 2, batch:     20] loss: 0.02892 time model: 0.02564 acc: 0.65156
[epoch: 2, batch:     40] loss: 0.02979 time model: 0.02563 acc: 0.64922
[epoch: 2, batch:     60] loss: 0.03048 time model: 0.02563 acc: 0.64427
[epoch: 2, batch:     80] loss: 0.03047 time model: 0.02564 acc: 0.63633
epoch:2 train loss: 0.0362578238166641 train acc: 0.5614783226723525 valid loss: 0.030721708120280196 valid acc: 0.6363989134652697
[epoch: 3, batch:     20] loss: 0.02897 time model: 0.03967 acc: 0.66094
[epoch: 3, batch:     40] loss: 0.03077 time model: 0.03965 acc: 0.64141
[epoch: 3, batch:     60] loss: 0.03125 time model: 0.03963 acc: 0.62760
[epoch: 3, batch:     80] loss: 0.03105 time model: 0.03963 acc: 0.62539
[epoch: 3, batch:    100] loss: 0.03091 time model: 0.03963 acc: 0.63062
[epoch: 3, batch:    120] loss: 0.03079 time model: 0.03963 acc: 0.63125
[epoch: 3, batch:    140] loss: 0.03076 time model: 0.03963 acc: 0.63281
[epoch: 3, batch:    160] loss: 0.03107 time model: 0.03963 acc: 0.62852
[epoch: 3, batch:    180] loss: 0.03119 time model: 0.03963 acc: 0.62760
[epoch: 3, batch:    200] loss: 0.03107 time model: 0.03963 acc: 0.62922
[epoch: 3, batch:    220] loss: 0.03109 time model: 0.03963 acc: 0.63040
[epoch: 3, batch:    240] loss: 0.03105 time model: 0.03962 acc: 0.63034
[epoch: 3, batch:    260] loss: 0.03068 time model: 0.03962 acc: 0.63353
[epoch: 3, batch:    280] loss: 0.03045 time model: 0.03962 acc: 0.63549
[epoch: 3, batch:    300] loss: 0.03050 time model: 0.03962 acc: 0.63573
[epoch: 3, batch:    320] loss: 0.03024 time model: 0.03962 acc: 0.63848
[epoch: 3, batch:    340] loss: 0.03017 time model: 0.03962 acc: 0.63833
[epoch: 3, batch:    360] loss: 0.03002 time model: 0.03962 acc: 0.64080
[epoch: 3, batch:    380] loss: 0.02988 time model: 0.03963 acc: 0.64309
[epoch: 3, batch:    400] loss: 0.02971 time model: 0.03963 acc: 0.64578
[epoch: 3, batch:    420] loss: 0.02969 time model: 0.03963 acc: 0.64539
[epoch: 3, batch:    440] loss: 0.02956 time model: 0.03963 acc: 0.64624
[epoch: 3, batch:    460] loss: 0.02964 time model: 0.03963 acc: 0.64606
[epoch: 3, batch:    480] loss: 0.02949 time model: 0.03962 acc: 0.64701
[epoch: 3, batch:    500] loss: 0.02935 time model: 0.03962 acc: 0.64875
[epoch: 3, batch:    520] loss: 0.02928 time model: 0.03962 acc: 0.64904
[epoch: 3, batch:    540] loss: 0.02914 time model: 0.03962 acc: 0.65023
[epoch: 3, batch:    560] loss: 0.02897 time model: 0.03962 acc: 0.65156
[epoch: 3, batch:    580] loss: 0.02894 time model: 0.03963 acc: 0.65226
[epoch: 3, batch:    600] loss: 0.02889 time model: 0.03963 acc: 0.65339
[epoch: 3, batch:    620] loss: 0.02883 time model: 0.03963 acc: 0.65439
[epoch: 3, batch:    640] loss: 0.02868 time model: 0.03963 acc: 0.65605
[epoch: 3, batch:    660] loss: 0.02861 time model: 0.03964 acc: 0.65691
[epoch: 3, batch:     20] loss: 0.02623 time model: 0.02566 acc: 0.70000
[epoch: 3, batch:     40] loss: 0.02600 time model: 0.02567 acc: 0.68984
[epoch: 3, batch:     60] loss: 0.02574 time model: 0.02565 acc: 0.69479
[epoch: 3, batch:     80] loss: 0.02651 time model: 0.02565 acc: 0.68711
epoch:3 train loss: 0.028610710791574498 train acc: 0.6569059464581852 valid loss: 0.026745960989523913 valid acc: 0.686069072564998
[epoch: 4, batch:     20] loss: 0.02334 time model: 0.03961 acc: 0.71719
[epoch: 4, batch:     40] loss: 0.02553 time model: 0.03954 acc: 0.70312
[epoch: 4, batch:     60] loss: 0.02516 time model: 0.03951 acc: 0.69844
[epoch: 4, batch:     80] loss: 0.02488 time model: 0.03949 acc: 0.70664
[epoch: 4, batch:    100] loss: 0.02509 time model: 0.03948 acc: 0.70063
[epoch: 4, batch:    120] loss: 0.02538 time model: 0.03946 acc: 0.70130
[epoch: 4, batch:    140] loss: 0.02579 time model: 0.03946 acc: 0.69665
[epoch: 4, batch:    160] loss: 0.02594 time model: 0.03945 acc: 0.69492
[epoch: 4, batch:    180] loss: 0.02610 time model: 0.03946 acc: 0.69219
[epoch: 4, batch:    200] loss: 0.02595 time model: 0.03947 acc: 0.69281
[epoch: 4, batch:    220] loss: 0.02567 time model: 0.03948 acc: 0.69389
[epoch: 4, batch:    240] loss: 0.02548 time model: 0.03949 acc: 0.69557
[epoch: 4, batch:    260] loss: 0.02544 time model: 0.03949 acc: 0.69567
[epoch: 4, batch:    280] loss: 0.02516 time model: 0.03950 acc: 0.69699
[epoch: 4, batch:    300] loss: 0.02502 time model: 0.03950 acc: 0.69885
[epoch: 4, batch:    320] loss: 0.02522 time model: 0.03950 acc: 0.69834
[epoch: 4, batch:    340] loss: 0.02532 time model: 0.03950 acc: 0.69779
[epoch: 4, batch:    360] loss: 0.02518 time model: 0.03950 acc: 0.69861
[epoch: 4, batch:    380] loss: 0.02516 time model: 0.03949 acc: 0.69910
[epoch: 4, batch:    400] loss: 0.02511 time model: 0.03949 acc: 0.70016
[epoch: 4, batch:    420] loss: 0.02515 time model: 0.03949 acc: 0.70007
[epoch: 4, batch:    440] loss: 0.02527 time model: 0.03949 acc: 0.69929
[epoch: 4, batch:    460] loss: 0.02535 time model: 0.03949 acc: 0.69857
[epoch: 4, batch:    480] loss: 0.02531 time model: 0.03949 acc: 0.69928
[epoch: 4, batch:    500] loss: 0.02532 time model: 0.03948 acc: 0.69950
[epoch: 4, batch:    520] loss: 0.02519 time model: 0.03948 acc: 0.70084
[epoch: 4, batch:    540] loss: 0.02514 time model: 0.03948 acc: 0.70104
[epoch: 4, batch:    560] loss: 0.02509 time model: 0.03947 acc: 0.70134
[epoch: 4, batch:    580] loss: 0.02514 time model: 0.03947 acc: 0.70070
[epoch: 4, batch:    600] loss: 0.02509 time model: 0.03947 acc: 0.70052
[epoch: 4, batch:    620] loss: 0.02509 time model: 0.03947 acc: 0.70096
[epoch: 4, batch:    640] loss: 0.02501 time model: 0.03947 acc: 0.70151
[epoch: 4, batch:    660] loss: 0.02506 time model: 0.03948 acc: 0.70111
[epoch: 4, batch:     20] loss: 0.02858 time model: 0.02555 acc: 0.64062
[epoch: 4, batch:     40] loss: 0.02738 time model: 0.02553 acc: 0.64844
[epoch: 4, batch:     60] loss: 0.02750 time model: 0.02552 acc: 0.65938
[epoch: 4, batch:     80] loss: 0.02764 time model: 0.02552 acc: 0.66172
epoch:4 train loss: 0.025062138299698007 train acc: 0.7011134802179578 valid loss: 0.02758857015727054 valid acc: 0.663174233604967
[epoch: 5, batch:     20] loss: 0.02346 time model: 0.03944 acc: 0.71719
[epoch: 5, batch:     40] loss: 0.02425 time model: 0.03944 acc: 0.70937
[epoch: 5, batch:     60] loss: 0.02396 time model: 0.03944 acc: 0.71875
[epoch: 5, batch:     80] loss: 0.02372 time model: 0.03945 acc: 0.72266
[epoch: 5, batch:    100] loss: 0.02401 time model: 0.03944 acc: 0.72000
[epoch: 5, batch:    120] loss: 0.02359 time model: 0.03944 acc: 0.72344
[epoch: 5, batch:    140] loss: 0.02348 time model: 0.03945 acc: 0.72545
[epoch: 5, batch:    160] loss: 0.02319 time model: 0.03945 acc: 0.72695
[epoch: 5, batch:    180] loss: 0.02285 time model: 0.03945 acc: 0.72934
[epoch: 5, batch:    200] loss: 0.02318 time model: 0.03945 acc: 0.72391
[epoch: 5, batch:    220] loss: 0.02321 time model: 0.03944 acc: 0.72259
[epoch: 5, batch:    240] loss: 0.02312 time model: 0.03944 acc: 0.72422
[epoch: 5, batch:    260] loss: 0.02314 time model: 0.03944 acc: 0.72344
[epoch: 5, batch:    280] loss: 0.02315 time model: 0.03944 acc: 0.72411
[epoch: 5, batch:    300] loss: 0.02310 time model: 0.03944 acc: 0.72344
[epoch: 5, batch:    320] loss: 0.02303 time model: 0.03944 acc: 0.72383
[epoch: 5, batch:    340] loss: 0.02300 time model: 0.03944 acc: 0.72408
[epoch: 5, batch:    360] loss: 0.02298 time model: 0.03944 acc: 0.72439
[epoch: 5, batch:    380] loss: 0.02284 time model: 0.03944 acc: 0.72664
[epoch: 5, batch:    400] loss: 0.02296 time model: 0.03945 acc: 0.72625
[epoch: 5, batch:    420] loss: 0.02292 time model: 0.03945 acc: 0.72708
[epoch: 5, batch:    440] loss: 0.02282 time model: 0.03945 acc: 0.72763
[epoch: 5, batch:    460] loss: 0.02277 time model: 0.03946 acc: 0.72846
[epoch: 5, batch:    480] loss: 0.02271 time model: 0.03946 acc: 0.72891
[epoch: 5, batch:    500] loss: 0.02263 time model: 0.03946 acc: 0.72962
[epoch: 5, batch:    520] loss: 0.02263 time model: 0.03946 acc: 0.72957
[epoch: 5, batch:    540] loss: 0.02260 time model: 0.03946 acc: 0.72986
[epoch: 5, batch:    560] loss: 0.02255 time model: 0.03947 acc: 0.73047
[epoch: 5, batch:    580] loss: 0.02255 time model: 0.03947 acc: 0.73039
[epoch: 5, batch:    600] loss: 0.02271 time model: 0.03948 acc: 0.72937
[epoch: 5, batch:    620] loss: 0.02270 time model: 0.03948 acc: 0.72969
[epoch: 5, batch:    640] loss: 0.02270 time model: 0.03949 acc: 0.72939
[epoch: 5, batch:    660] loss: 0.02271 time model: 0.03950 acc: 0.72945
[epoch: 5, batch:     20] loss: 0.02150 time model: 0.02568 acc: 0.74687
[epoch: 5, batch:     40] loss: 0.02272 time model: 0.02568 acc: 0.74219
[epoch: 5, batch:     60] loss: 0.02285 time model: 0.02567 acc: 0.73646
[epoch: 5, batch:     80] loss: 0.02254 time model: 0.02565 acc: 0.73750
epoch:5 train loss: 0.02271311534041337 train acc: 0.7294479981047145 valid loss: 0.02263918719089998 valid acc: 0.7376794722545595
[epoch: 6, batch:     20] loss: 0.02105 time model: 0.03951 acc: 0.73438
[epoch: 6, batch:     40] loss: 0.02197 time model: 0.03951 acc: 0.73125
[epoch: 6, batch:     60] loss: 0.02149 time model: 0.03953 acc: 0.73385
[epoch: 6, batch:     80] loss: 0.02127 time model: 0.03953 acc: 0.73711
[epoch: 6, batch:    100] loss: 0.02067 time model: 0.03952 acc: 0.74531
[epoch: 6, batch:    120] loss: 0.02089 time model: 0.03951 acc: 0.74609
[epoch: 6, batch:    140] loss: 0.02092 time model: 0.03952 acc: 0.74621
[epoch: 6, batch:    160] loss: 0.02108 time model: 0.03954 acc: 0.74355
[epoch: 6, batch:    180] loss: 0.02103 time model: 0.03955 acc: 0.74427
[epoch: 6, batch:    200] loss: 0.02120 time model: 0.03955 acc: 0.74438
[epoch: 6, batch:    220] loss: 0.02109 time model: 0.03956 acc: 0.74560
[epoch: 6, batch:    240] loss: 0.02108 time model: 0.03956 acc: 0.74609
[epoch: 6, batch:    260] loss: 0.02100 time model: 0.03956 acc: 0.74724
[epoch: 6, batch:    280] loss: 0.02094 time model: 0.03957 acc: 0.74777
[epoch: 6, batch:    300] loss: 0.02097 time model: 0.03957 acc: 0.74760
[epoch: 6, batch:    320] loss: 0.02092 time model: 0.03957 acc: 0.74912
[epoch: 6, batch:    340] loss: 0.02093 time model: 0.03958 acc: 0.74972
[epoch: 6, batch:    360] loss: 0.02089 time model: 0.03958 acc: 0.74852
[epoch: 6, batch:    380] loss: 0.02090 time model: 0.03958 acc: 0.74786
[epoch: 6, batch:    400] loss: 0.02076 time model: 0.03959 acc: 0.74938
[epoch: 6, batch:    420] loss: 0.02074 time model: 0.03959 acc: 0.75000
[epoch: 6, batch:    440] loss: 0.02074 time model: 0.03959 acc: 0.75064
[epoch: 6, batch:    460] loss: 0.02074 time model: 0.03960 acc: 0.75149
[epoch: 6, batch:    480] loss: 0.02076 time model: 0.03961 acc: 0.75124
[epoch: 6, batch:    500] loss: 0.02076 time model: 0.03962 acc: 0.75144
[epoch: 6, batch:    520] loss: 0.02070 time model: 0.03963 acc: 0.75204
[epoch: 6, batch:    540] loss: 0.02068 time model: 0.03963 acc: 0.75162
[epoch: 6, batch:    560] loss: 0.02071 time model: 0.03963 acc: 0.75140
[epoch: 6, batch:    580] loss: 0.02072 time model: 0.03963 acc: 0.75145
[epoch: 6, batch:    600] loss: 0.02073 time model: 0.03963 acc: 0.75172
[epoch: 6, batch:    620] loss: 0.02073 time model: 0.03963 acc: 0.75202
[epoch: 6, batch:    640] loss: 0.02075 time model: 0.03963 acc: 0.75225
[epoch: 6, batch:    660] loss: 0.02082 time model: 0.03964 acc: 0.75153
[epoch: 6, batch:     20] loss: 0.02240 time model: 0.02571 acc: 0.72188
[epoch: 6, batch:     40] loss: 0.02264 time model: 0.02569 acc: 0.72109
[epoch: 6, batch:     60] loss: 0.02262 time model: 0.02570 acc: 0.72656
[epoch: 6, batch:     80] loss: 0.02269 time model: 0.02568 acc: 0.72813
epoch:6 train loss: 0.02081860974766411 train acc: 0.7515280739161336 valid loss: 0.022898156107697878 valid acc: 0.7275902211874272
[epoch: 7, batch:     20] loss: 0.02054 time model: 0.03968 acc: 0.75625
[epoch: 7, batch:     40] loss: 0.02107 time model: 0.03971 acc: 0.75000
[epoch: 7, batch:     60] loss: 0.01994 time model: 0.03971 acc: 0.75938
[epoch: 7, batch:     80] loss: 0.01937 time model: 0.03972 acc: 0.76641
[epoch: 7, batch:    100] loss: 0.01969 time model: 0.03972 acc: 0.76156
[epoch: 7, batch:    120] loss: 0.02014 time model: 0.03972 acc: 0.75781
[epoch: 7, batch:    140] loss: 0.02020 time model: 0.03972 acc: 0.76004
[epoch: 7, batch:    160] loss: 0.02005 time model: 0.03971 acc: 0.75742
[epoch: 7, batch:    180] loss: 0.01989 time model: 0.03972 acc: 0.75712
[epoch: 7, batch:    200] loss: 0.01962 time model: 0.03972 acc: 0.75859
[epoch: 7, batch:    220] loss: 0.01959 time model: 0.03972 acc: 0.75980
[epoch: 7, batch:    240] loss: 0.01950 time model: 0.03972 acc: 0.76003
[epoch: 7, batch:    260] loss: 0.01948 time model: 0.03972 acc: 0.76178
[epoch: 7, batch:    280] loss: 0.01950 time model: 0.03972 acc: 0.76172
[epoch: 7, batch:    300] loss: 0.01951 time model: 0.03972 acc: 0.76219
[epoch: 7, batch:    320] loss: 0.01943 time model: 0.03972 acc: 0.76152
[epoch: 7, batch:    340] loss: 0.01942 time model: 0.03971 acc: 0.76195
[epoch: 7, batch:    360] loss: 0.01946 time model: 0.03971 acc: 0.76189
[epoch: 7, batch:    380] loss: 0.01950 time model: 0.03970 acc: 0.76209
[epoch: 7, batch:    400] loss: 0.01959 time model: 0.03970 acc: 0.76102
[epoch: 7, batch:    420] loss: 0.01953 time model: 0.03970 acc: 0.76109
[epoch: 7, batch:    440] loss: 0.01943 time model: 0.03969 acc: 0.76271
[epoch: 7, batch:    460] loss: 0.01941 time model: 0.03969 acc: 0.76236
[epoch: 7, batch:    480] loss: 0.01946 time model: 0.03969 acc: 0.76107
[epoch: 7, batch:    500] loss: 0.01964 time model: 0.03969 acc: 0.75913
[epoch: 7, batch:    520] loss: 0.01967 time model: 0.03968 acc: 0.75871
[epoch: 7, batch:    540] loss: 0.01965 time model: 0.03968 acc: 0.75862
[epoch: 7, batch:    560] loss: 0.01967 time model: 0.03968 acc: 0.75871
[epoch: 7, batch:    580] loss: 0.01960 time model: 0.03968 acc: 0.75970
[epoch: 7, batch:    600] loss: 0.01956 time model: 0.03968 acc: 0.76021
[epoch: 7, batch:    620] loss: 0.01951 time model: 0.03968 acc: 0.76124
[epoch: 7, batch:    640] loss: 0.01949 time model: 0.03968 acc: 0.76216
[epoch: 7, batch:    660] loss: 0.01955 time model: 0.03969 acc: 0.76176
[epoch: 7, batch:     20] loss: 0.02057 time model: 0.02564 acc: 0.76406
[epoch: 7, batch:     40] loss: 0.02229 time model: 0.02559 acc: 0.74531
[epoch: 7, batch:     60] loss: 0.02246 time model: 0.02559 acc: 0.74010
[epoch: 7, batch:     80] loss: 0.02208 time model: 0.02558 acc: 0.74180
epoch:7 train loss: 0.019546566299566356 train acc: 0.7617626154939587 valid loss: 0.022214681245487793 valid acc: 0.7415599534342259
[epoch: 8, batch:     20] loss: 0.01614 time model: 0.03958 acc: 0.80781
[epoch: 8, batch:     40] loss: 0.01670 time model: 0.03955 acc: 0.79844
[epoch: 8, batch:     60] loss: 0.01721 time model: 0.03953 acc: 0.78802
[epoch: 8, batch:     80] loss: 0.01796 time model: 0.03950 acc: 0.78047
[epoch: 8, batch:    100] loss: 0.01776 time model: 0.03948 acc: 0.78219
[epoch: 8, batch:    120] loss: 0.01775 time model: 0.03948 acc: 0.78333
[epoch: 8, batch:    140] loss: 0.01791 time model: 0.03947 acc: 0.78192
[epoch: 8, batch:    160] loss: 0.01783 time model: 0.03947 acc: 0.78105
[epoch: 8, batch:    180] loss: 0.01776 time model: 0.03947 acc: 0.78177
[epoch: 8, batch:    200] loss: 0.01784 time model: 0.03946 acc: 0.78297
[epoch: 8, batch:    220] loss: 0.01787 time model: 0.03945 acc: 0.78381
[epoch: 8, batch:    240] loss: 0.01794 time model: 0.03945 acc: 0.78138
[epoch: 8, batch:    260] loss: 0.01789 time model: 0.03945 acc: 0.78209
[epoch: 8, batch:    280] loss: 0.01782 time model: 0.03945 acc: 0.78203
[epoch: 8, batch:    300] loss: 0.01772 time model: 0.03946 acc: 0.78281
[epoch: 8, batch:    320] loss: 0.01786 time model: 0.03946 acc: 0.78105
[epoch: 8, batch:    340] loss: 0.01810 time model: 0.03946 acc: 0.77831
[epoch: 8, batch:    360] loss: 0.01804 time model: 0.03946 acc: 0.77951
[epoch: 8, batch:    380] loss: 0.01810 time model: 0.03946 acc: 0.77936
[epoch: 8, batch:    400] loss: 0.01826 time model: 0.03945 acc: 0.77820
[epoch: 8, batch:    420] loss: 0.01832 time model: 0.03945 acc: 0.77753
[epoch: 8, batch:    440] loss: 0.01829 time model: 0.03945 acc: 0.77784
[epoch: 8, batch:    460] loss: 0.01829 time model: 0.03945 acc: 0.77846
[epoch: 8, batch:    480] loss: 0.01831 time model: 0.03945 acc: 0.77819
[epoch: 8, batch:    500] loss: 0.01829 time model: 0.03945 acc: 0.77838
[epoch: 8, batch:    520] loss: 0.01829 time model: 0.03945 acc: 0.77885
[epoch: 8, batch:    540] loss: 0.01826 time model: 0.03945 acc: 0.77934
[epoch: 8, batch:    560] loss: 0.01824 time model: 0.03945 acc: 0.77907
[epoch: 8, batch:    580] loss: 0.01817 time model: 0.03945 acc: 0.77963
[epoch: 8, batch:    600] loss: 0.01825 time model: 0.03944 acc: 0.77906
[epoch: 8, batch:    620] loss: 0.01829 time model: 0.03944 acc: 0.77843
[epoch: 8, batch:    640] loss: 0.01827 time model: 0.03944 acc: 0.77817
[epoch: 8, batch:    660] loss: 0.01830 time model: 0.03945 acc: 0.77854
[epoch: 8, batch:     20] loss: 0.02053 time model: 0.02546 acc: 0.75000
[epoch: 8, batch:     40] loss: 0.02138 time model: 0.02547 acc: 0.74531
[epoch: 8, batch:     60] loss: 0.02193 time model: 0.02548 acc: 0.73958
[epoch: 8, batch:     80] loss: 0.02198 time model: 0.02549 acc: 0.73867
epoch:8 train loss: 0.01830277192637808 train acc: 0.7785358919687277 valid loss: 0.022022646101214047 valid acc: 0.7400077609623593
[epoch: 9, batch:     20] loss: 0.01812 time model: 0.03944 acc: 0.79219
[epoch: 9, batch:     40] loss: 0.01778 time model: 0.03944 acc: 0.78359
[epoch: 9, batch:     60] loss: 0.01698 time model: 0.03948 acc: 0.79167
[epoch: 9, batch:     80] loss: 0.01687 time model: 0.03949 acc: 0.79648
[epoch: 9, batch:    100] loss: 0.01654 time model: 0.03949 acc: 0.80125
[epoch: 9, batch:    120] loss: 0.01661 time model: 0.03949 acc: 0.80104
[epoch: 9, batch:    140] loss: 0.01662 time model: 0.03949 acc: 0.80045
[epoch: 9, batch:    160] loss: 0.01662 time model: 0.03949 acc: 0.79941
[epoch: 9, batch:    180] loss: 0.01657 time model: 0.03949 acc: 0.80139
[epoch: 9, batch:    200] loss: 0.01653 time model: 0.03949 acc: 0.80172
[epoch: 9, batch:    220] loss: 0.01650 time model: 0.03949 acc: 0.80312
[epoch: 9, batch:    240] loss: 0.01668 time model: 0.03949 acc: 0.80182
[epoch: 9, batch:    260] loss: 0.01685 time model: 0.03950 acc: 0.80048
[epoch: 9, batch:    280] loss: 0.01693 time model: 0.03951 acc: 0.79933
[epoch: 9, batch:    300] loss: 0.01683 time model: 0.03951 acc: 0.79896
[epoch: 9, batch:    320] loss: 0.01689 time model: 0.03952 acc: 0.79775
[epoch: 9, batch:    340] loss: 0.01697 time model: 0.03952 acc: 0.79632
[epoch: 9, batch:    360] loss: 0.01699 time model: 0.03953 acc: 0.79523
[epoch: 9, batch:    380] loss: 0.01708 time model: 0.03953 acc: 0.79243
[epoch: 9, batch:    400] loss: 0.01703 time model: 0.03954 acc: 0.79312
[epoch: 9, batch:    420] loss: 0.01694 time model: 0.03954 acc: 0.79368
[epoch: 9, batch:    440] loss: 0.01696 time model: 0.03955 acc: 0.79425
[epoch: 9, batch:    460] loss: 0.01702 time model: 0.03956 acc: 0.79314
[epoch: 9, batch:    480] loss: 0.01692 time model: 0.03957 acc: 0.79408
[epoch: 9, batch:    500] loss: 0.01690 time model: 0.03957 acc: 0.79406
[epoch: 9, batch:    520] loss: 0.01690 time model: 0.03958 acc: 0.79381
[epoch: 9, batch:    540] loss: 0.01698 time model: 0.03958 acc: 0.79323
[epoch: 9, batch:    560] loss: 0.01699 time model: 0.03958 acc: 0.79353
[epoch: 9, batch:    580] loss: 0.01697 time model: 0.03959 acc: 0.79343
[epoch: 9, batch:    600] loss: 0.01696 time model: 0.03959 acc: 0.79307
[epoch: 9, batch:    620] loss: 0.01693 time model: 0.03959 acc: 0.79330
[epoch: 9, batch:    640] loss: 0.01692 time model: 0.03960 acc: 0.79331
[epoch: 9, batch:    660] loss: 0.01688 time model: 0.03961 acc: 0.79384
[epoch: 9, batch:     20] loss: 0.02498 time model: 0.02573 acc: 0.70469
[epoch: 9, batch:     40] loss: 0.02630 time model: 0.02575 acc: 0.69844
[epoch: 9, batch:     60] loss: 0.02593 time model: 0.02573 acc: 0.69844
[epoch: 9, batch:     80] loss: 0.02538 time model: 0.02570 acc: 0.70000
epoch:9 train loss: 0.016877992777738545 train acc: 0.7938403221985312 valid loss: 0.02551286006371643 valid acc: 0.7000388048117967
[epoch: 10, batch:     20] loss: 0.01680 time model: 0.03953 acc: 0.79375
[epoch: 10, batch:     40] loss: 0.01637 time model: 0.03953 acc: 0.79688
[epoch: 10, batch:     60] loss: 0.01634 time model: 0.03951 acc: 0.80000
[epoch: 10, batch:     80] loss: 0.01611 time model: 0.03951 acc: 0.80000
[epoch: 10, batch:    100] loss: 0.01581 time model: 0.03951 acc: 0.80125
[epoch: 10, batch:    120] loss: 0.01565 time model: 0.03951 acc: 0.80417
[epoch: 10, batch:    140] loss: 0.01545 time model: 0.03952 acc: 0.80848
[epoch: 10, batch:    160] loss: 0.01535 time model: 0.03952 acc: 0.80918
[epoch: 10, batch:    180] loss: 0.01527 time model: 0.03953 acc: 0.81163
[epoch: 10, batch:    200] loss: 0.01534 time model: 0.03953 acc: 0.81219
[epoch: 10, batch:    220] loss: 0.01541 time model: 0.03953 acc: 0.81065
[epoch: 10, batch:    240] loss: 0.01545 time model: 0.03953 acc: 0.81081
[epoch: 10, batch:    260] loss: 0.01542 time model: 0.03954 acc: 0.81154
[epoch: 10, batch:    280] loss: 0.01556 time model: 0.03954 acc: 0.81060
[epoch: 10, batch:    300] loss: 0.01563 time model: 0.03954 acc: 0.80937
[epoch: 10, batch:    320] loss: 0.01564 time model: 0.03953 acc: 0.80898
[epoch: 10, batch:    340] loss: 0.01557 time model: 0.03953 acc: 0.80956
[epoch: 10, batch:    360] loss: 0.01565 time model: 0.03953 acc: 0.80833
[epoch: 10, batch:    380] loss: 0.01578 time model: 0.03953 acc: 0.80543
[epoch: 10, batch:    400] loss: 0.01589 time model: 0.03953 acc: 0.80492
[epoch: 10, batch:    420] loss: 0.01588 time model: 0.03953 acc: 0.80454
[epoch: 10, batch:    440] loss: 0.01580 time model: 0.03952 acc: 0.80604
[epoch: 10, batch:    460] loss: 0.01585 time model: 0.03952 acc: 0.80571
[epoch: 10, batch:    480] loss: 0.01600 time model: 0.03952 acc: 0.80488
[epoch: 10, batch:    500] loss: 0.01608 time model: 0.03952 acc: 0.80350
[epoch: 10, batch:    520] loss: 0.01598 time model: 0.03951 acc: 0.80421
[epoch: 10, batch:    540] loss: 0.01594 time model: 0.03951 acc: 0.80509
[epoch: 10, batch:    560] loss: 0.01589 time model: 0.03951 acc: 0.80597
[epoch: 10, batch:    580] loss: 0.01594 time model: 0.03951 acc: 0.80560
[epoch: 10, batch:    600] loss: 0.01596 time model: 0.03951 acc: 0.80542
[epoch: 10, batch:    620] loss: 0.01587 time model: 0.03950 acc: 0.80675
[epoch: 10, batch:    640] loss: 0.01585 time model: 0.03950 acc: 0.80674
[epoch: 10, batch:    660] loss: 0.01581 time model: 0.03951 acc: 0.80730
[epoch: 10, batch:     20] loss: 0.02045 time model: 0.02549 acc: 0.74844
[epoch: 10, batch:     40] loss: 0.02031 time model: 0.02550 acc: 0.76016
[epoch: 10, batch:     60] loss: 0.02077 time model: 0.02550 acc: 0.75417
[epoch: 10, batch:     80] loss: 0.02057 time model: 0.02550 acc: 0.75664
epoch:10 train loss: 0.015812679199606456 train acc: 0.8072968490878939 valid loss: 0.02068921561661181 valid acc: 0.757081878152891
