[epoch: 1, batch:     20] loss: 0.16236 time model: 0.02549 acc: 0.08438
[epoch: 1, batch:     40] loss: 0.15455 time model: 0.02416 acc: 0.10781
[epoch: 1, batch:     60] loss: 0.14952 time model: 0.02377 acc: 0.12917
[epoch: 1, batch:     80] loss: 0.14314 time model: 0.02344 acc: 0.15703
[epoch: 1, batch:    100] loss: 0.13991 time model: 0.02335 acc: 0.16875
[epoch: 1, batch:    120] loss: 0.13669 time model: 0.02321 acc: 0.18542
[epoch: 1, batch:    140] loss: 0.13327 time model: 0.02316 acc: 0.20179
[epoch: 1, batch:    160] loss: 0.13204 time model: 0.02307 acc: 0.20859
[epoch: 1, batch:    180] loss: 0.12980 time model: 0.02302 acc: 0.22049
[epoch: 1, batch:    200] loss: 0.12933 time model: 0.02296 acc: 0.22656
[epoch: 1, batch:    220] loss: 0.12828 time model: 0.02294 acc: 0.23324
[epoch: 1, batch:    240] loss: 0.12735 time model: 0.02294 acc: 0.23828
[epoch: 1, batch:    260] loss: 0.12608 time model: 0.02290 acc: 0.24760
[epoch: 1, batch:    280] loss: 0.12499 time model: 0.02288 acc: 0.25312
[epoch: 1, batch:    300] loss: 0.12353 time model: 0.02286 acc: 0.26229
[epoch: 1, batch:    320] loss: 0.12173 time model: 0.02285 acc: 0.27129
[epoch: 1, batch:    340] loss: 0.12052 time model: 0.02283 acc: 0.27978
[epoch: 1, batch:    360] loss: 0.11944 time model: 0.02281 acc: 0.28437
[epoch: 1, batch:    380] loss: 0.11853 time model: 0.02279 acc: 0.29013
[epoch: 1, batch:    400] loss: 0.11772 time model: 0.02278 acc: 0.29500
[epoch: 1, batch:    420] loss: 0.11653 time model: 0.02276 acc: 0.30119
[epoch: 1, batch:    440] loss: 0.11528 time model: 0.02275 acc: 0.30639
[epoch: 1, batch:    460] loss: 0.11461 time model: 0.02274 acc: 0.31087
[epoch: 1, batch:    480] loss: 0.11415 time model: 0.02272 acc: 0.31458
[epoch: 1, batch:    500] loss: 0.11318 time model: 0.02272 acc: 0.32100
[epoch: 1, batch:    520] loss: 0.11220 time model: 0.02272 acc: 0.32560
[epoch: 1, batch:    540] loss: 0.11123 time model: 0.02271 acc: 0.33009
[epoch: 1, batch:    560] loss: 0.11059 time model: 0.02271 acc: 0.33248
[epoch: 1, batch:    580] loss: 0.10974 time model: 0.02269 acc: 0.33793
[epoch: 1, batch:    600] loss: 0.10875 time model: 0.02270 acc: 0.34333
[epoch: 1, batch:    620] loss: 0.10777 time model: 0.02270 acc: 0.34859
[epoch: 1, batch:    640] loss: 0.10678 time model: 0.02270 acc: 0.35469
[epoch: 1, batch:    660] loss: 0.10592 time model: 0.02269 acc: 0.35938
[epoch: 1, batch:    680] loss: 0.10496 time model: 0.02268 acc: 0.36461
[epoch: 1, batch:    700] loss: 0.10430 time model: 0.02268 acc: 0.36911
[epoch: 1, batch:    720] loss: 0.10358 time model: 0.02267 acc: 0.37422
[epoch: 1, batch:    740] loss: 0.10291 time model: 0.02267 acc: 0.37812
[epoch: 1, batch:    760] loss: 0.10250 time model: 0.02267 acc: 0.38059
[epoch: 1, batch:    780] loss: 0.10174 time model: 0.02266 acc: 0.38470
[epoch: 1, batch:    800] loss: 0.10096 time model: 0.02267 acc: 0.38945
[epoch: 1, batch:    820] loss: 0.10028 time model: 0.02267 acc: 0.39383
[epoch: 1, batch:    840] loss: 0.09967 time model: 0.02266 acc: 0.39777
[epoch: 1, batch:    860] loss: 0.09883 time model: 0.02266 acc: 0.40254
[epoch: 1, batch:    880] loss: 0.09834 time model: 0.02265 acc: 0.40575
[epoch: 1, batch:    900] loss: 0.09776 time model: 0.02264 acc: 0.40889
[epoch: 1, batch:    920] loss: 0.09705 time model: 0.02264 acc: 0.41298
[epoch: 1, batch:    940] loss: 0.09636 time model: 0.02264 acc: 0.41735
[epoch: 1, batch:    960] loss: 0.09573 time model: 0.02264 acc: 0.42090
[epoch: 1, batch:    980] loss: 0.09528 time model: 0.02263 acc: 0.42366
[epoch: 1, batch:   1000] loss: 0.09472 time model: 0.02264 acc: 0.42638
[epoch: 1, batch:   1020] loss: 0.09422 time model: 0.02264 acc: 0.42923
[epoch: 1, batch:   1040] loss: 0.09360 time model: 0.02264 acc: 0.43197
[epoch: 1, batch:   1060] loss: 0.09300 time model: 0.02264 acc: 0.43597
[epoch: 1, batch:   1080] loss: 0.09239 time model: 0.02264 acc: 0.43987
[epoch: 1, batch:   1100] loss: 0.09199 time model: 0.02265 acc: 0.44250
[epoch: 1, batch:   1120] loss: 0.09155 time model: 0.02265 acc: 0.44475
[epoch: 1, batch:   1140] loss: 0.09105 time model: 0.02265 acc: 0.44825
[epoch: 1, batch:   1160] loss: 0.09040 time model: 0.02265 acc: 0.45226
[epoch: 1, batch:   1180] loss: 0.08992 time model: 0.02265 acc: 0.45535
[epoch: 1, batch:   1200] loss: 0.08934 time model: 0.02265 acc: 0.45922
[epoch: 1, batch:   1220] loss: 0.08896 time model: 0.02266 acc: 0.46153
[epoch: 1, batch:   1240] loss: 0.08856 time model: 0.02266 acc: 0.46381
[epoch: 1, batch:   1260] loss: 0.08823 time model: 0.02266 acc: 0.46558
[epoch: 1, batch:   1280] loss: 0.08768 time model: 0.02266 acc: 0.46909
[epoch: 1, batch:   1300] loss: 0.08735 time model: 0.02266 acc: 0.47212
[epoch: 1, batch:   1320] loss: 0.08705 time model: 0.02265 acc: 0.47486
[epoch: 1, batch:     20] loss: 0.04582 time model: 0.01594 acc: 0.71875
[epoch: 1, batch:     40] loss: 0.04894 time model: 0.01591 acc: 0.71719
[epoch: 1, batch:     60] loss: 0.04719 time model: 0.01588 acc: 0.72813
[epoch: 1, batch:     80] loss: 0.04688 time model: 0.01583 acc: 0.73125
[epoch: 1, batch:    100] loss: 0.04753 time model: 0.01581 acc: 0.72937
[epoch: 1, batch:    120] loss: 0.04781 time model: 0.01578 acc: 0.72760
[epoch: 1, batch:    140] loss: 0.04834 time model: 0.01575 acc: 0.72455
[epoch: 1, batch:    160] loss: 0.04881 time model: 0.01574 acc: 0.72227
epoch:1 train loss: 0.08705466260307916 train acc: 0.47486377635631366 valid loss: 0.04889691258799812 valid acc: 0.7225455956538611
[epoch: 2, batch:     20] loss: 0.05637 time model: 0.02257 acc: 0.65938
[epoch: 2, batch:     40] loss: 0.05814 time model: 0.02282 acc: 0.65156
[epoch: 2, batch:     60] loss: 0.05657 time model: 0.02283 acc: 0.66771
[epoch: 2, batch:     80] loss: 0.05584 time model: 0.02281 acc: 0.67344
[epoch: 2, batch:    100] loss: 0.05514 time model: 0.02285 acc: 0.67125
[epoch: 2, batch:    120] loss: 0.05401 time model: 0.02282 acc: 0.67448
[epoch: 2, batch:    140] loss: 0.05457 time model: 0.02278 acc: 0.67589
[epoch: 2, batch:    160] loss: 0.05360 time model: 0.02283 acc: 0.68164
[epoch: 2, batch:    180] loss: 0.05380 time model: 0.02281 acc: 0.68021
[epoch: 2, batch:    200] loss: 0.05470 time model: 0.02284 acc: 0.67812
[epoch: 2, batch:    220] loss: 0.05469 time model: 0.02287 acc: 0.68125
[epoch: 2, batch:    240] loss: 0.05492 time model: 0.02287 acc: 0.67812
[epoch: 2, batch:    260] loss: 0.05468 time model: 0.02289 acc: 0.67933
[epoch: 2, batch:    280] loss: 0.05408 time model: 0.02290 acc: 0.68147
[epoch: 2, batch:    300] loss: 0.05361 time model: 0.02288 acc: 0.68625
[epoch: 2, batch:    320] loss: 0.05344 time model: 0.02289 acc: 0.68672
[epoch: 2, batch:    340] loss: 0.05314 time model: 0.02289 acc: 0.68897
[epoch: 2, batch:    360] loss: 0.05309 time model: 0.02289 acc: 0.69063
[epoch: 2, batch:    380] loss: 0.05307 time model: 0.02289 acc: 0.69276
[epoch: 2, batch:    400] loss: 0.05280 time model: 0.02288 acc: 0.69500
[epoch: 2, batch:    420] loss: 0.05258 time model: 0.02290 acc: 0.69821
[epoch: 2, batch:    440] loss: 0.05235 time model: 0.02289 acc: 0.70043
[epoch: 2, batch:    460] loss: 0.05193 time model: 0.02290 acc: 0.70217
[epoch: 2, batch:    480] loss: 0.05155 time model: 0.02290 acc: 0.70495
[epoch: 2, batch:    500] loss: 0.05098 time model: 0.02289 acc: 0.70788
[epoch: 2, batch:    520] loss: 0.05106 time model: 0.02289 acc: 0.70757
[epoch: 2, batch:    540] loss: 0.05080 time model: 0.02289 acc: 0.70926
[epoch: 2, batch:    560] loss: 0.05070 time model: 0.02288 acc: 0.70859
[epoch: 2, batch:    580] loss: 0.05038 time model: 0.02289 acc: 0.71142
[epoch: 2, batch:    600] loss: 0.05014 time model: 0.02288 acc: 0.71208
[epoch: 2, batch:    620] loss: 0.04963 time model: 0.02288 acc: 0.71502
[epoch: 2, batch:    640] loss: 0.04935 time model: 0.02288 acc: 0.71699
[epoch: 2, batch:    660] loss: 0.04898 time model: 0.02288 acc: 0.72008
[epoch: 2, batch:    680] loss: 0.04879 time model: 0.02287 acc: 0.72068
[epoch: 2, batch:    700] loss: 0.04863 time model: 0.02287 acc: 0.72223
[epoch: 2, batch:    720] loss: 0.04817 time model: 0.02288 acc: 0.72535
[epoch: 2, batch:    740] loss: 0.04793 time model: 0.02288 acc: 0.72720
[epoch: 2, batch:    760] loss: 0.04775 time model: 0.02288 acc: 0.72887
[epoch: 2, batch:    780] loss: 0.04750 time model: 0.02289 acc: 0.73061
[epoch: 2, batch:    800] loss: 0.04719 time model: 0.02288 acc: 0.73273
[epoch: 2, batch:    820] loss: 0.04708 time model: 0.02287 acc: 0.73392
[epoch: 2, batch:    840] loss: 0.04679 time model: 0.02288 acc: 0.73594
[epoch: 2, batch:    860] loss: 0.04661 time model: 0.02287 acc: 0.73685
[epoch: 2, batch:    880] loss: 0.04648 time model: 0.02287 acc: 0.73821
[epoch: 2, batch:    900] loss: 0.04617 time model: 0.02286 acc: 0.74062
[epoch: 2, batch:    920] loss: 0.04583 time model: 0.02287 acc: 0.74239
[epoch: 2, batch:    940] loss: 0.04561 time model: 0.02286 acc: 0.74388
[epoch: 2, batch:    960] loss: 0.04529 time model: 0.02286 acc: 0.74629
[epoch: 2, batch:    980] loss: 0.04505 time model: 0.02287 acc: 0.74777
[epoch: 2, batch:   1000] loss: 0.04481 time model: 0.02287 acc: 0.74906
[epoch: 2, batch:   1020] loss: 0.04452 time model: 0.02287 acc: 0.75074
[epoch: 2, batch:   1040] loss: 0.04405 time model: 0.02287 acc: 0.75367
[epoch: 2, batch:   1060] loss: 0.04369 time model: 0.02287 acc: 0.75601
[epoch: 2, batch:   1080] loss: 0.04357 time model: 0.02287 acc: 0.75660
[epoch: 2, batch:   1100] loss: 0.04348 time model: 0.02287 acc: 0.75733
[epoch: 2, batch:   1120] loss: 0.04334 time model: 0.02287 acc: 0.75809
[epoch: 2, batch:   1140] loss: 0.04318 time model: 0.02287 acc: 0.75932
[epoch: 2, batch:   1160] loss: 0.04306 time model: 0.02287 acc: 0.76018
[epoch: 2, batch:   1180] loss: 0.04284 time model: 0.02287 acc: 0.76149
[epoch: 2, batch:   1200] loss: 0.04262 time model: 0.02287 acc: 0.76292
[epoch: 2, batch:   1220] loss: 0.04251 time model: 0.02287 acc: 0.76388
[epoch: 2, batch:   1240] loss: 0.04232 time model: 0.02287 acc: 0.76487
[epoch: 2, batch:   1260] loss: 0.04225 time model: 0.02287 acc: 0.76577
[epoch: 2, batch:   1280] loss: 0.04196 time model: 0.02287 acc: 0.76763
[epoch: 2, batch:   1300] loss: 0.04189 time model: 0.02287 acc: 0.76832
[epoch: 2, batch:   1320] loss: 0.04172 time model: 0.02286 acc: 0.76958
[epoch: 2, batch:     20] loss: 0.03837 time model: 0.01584 acc: 0.79688
[epoch: 2, batch:     40] loss: 0.03635 time model: 0.01586 acc: 0.81406
[epoch: 2, batch:     60] loss: 0.03631 time model: 0.01587 acc: 0.80729
[epoch: 2, batch:     80] loss: 0.03531 time model: 0.01589 acc: 0.80781
[epoch: 2, batch:    100] loss: 0.03533 time model: 0.01588 acc: 0.80937
[epoch: 2, batch:    120] loss: 0.03670 time model: 0.01587 acc: 0.80365
[epoch: 2, batch:    140] loss: 0.03696 time model: 0.01583 acc: 0.80268
[epoch: 2, batch:    160] loss: 0.03713 time model: 0.01582 acc: 0.80195
epoch:2 train loss: 0.041715673655730465 train acc: 0.7695806680881307 valid loss: 0.03715748613108301 valid acc: 0.8020954598370198
[epoch: 3, batch:     20] loss: 0.03047 time model: 0.02286 acc: 0.83750
[epoch: 3, batch:     40] loss: 0.03299 time model: 0.02274 acc: 0.81719
[epoch: 3, batch:     60] loss: 0.02998 time model: 0.02287 acc: 0.83646
[epoch: 3, batch:     80] loss: 0.02996 time model: 0.02279 acc: 0.83594
[epoch: 3, batch:    100] loss: 0.02964 time model: 0.02283 acc: 0.83750
[epoch: 3, batch:    120] loss: 0.02900 time model: 0.02283 acc: 0.83958
[epoch: 3, batch:    140] loss: 0.02898 time model: 0.02286 acc: 0.84062
[epoch: 3, batch:    160] loss: 0.02953 time model: 0.02290 acc: 0.83945
[epoch: 3, batch:    180] loss: 0.03007 time model: 0.02287 acc: 0.83854
[epoch: 3, batch:    200] loss: 0.02964 time model: 0.02291 acc: 0.84156
[epoch: 3, batch:    220] loss: 0.02907 time model: 0.02292 acc: 0.84602
[epoch: 3, batch:    240] loss: 0.02813 time model: 0.02292 acc: 0.85260
[epoch: 3, batch:    260] loss: 0.02776 time model: 0.02295 acc: 0.85409
[epoch: 3, batch:    280] loss: 0.02771 time model: 0.02296 acc: 0.85536
[epoch: 3, batch:    300] loss: 0.02775 time model: 0.02297 acc: 0.85479
[epoch: 3, batch:    320] loss: 0.02772 time model: 0.02299 acc: 0.85527
[epoch: 3, batch:    340] loss: 0.02735 time model: 0.02298 acc: 0.85680
[epoch: 3, batch:    360] loss: 0.02689 time model: 0.02298 acc: 0.85938
[epoch: 3, batch:    380] loss: 0.02674 time model: 0.02298 acc: 0.86086
[epoch: 3, batch:    400] loss: 0.02668 time model: 0.02298 acc: 0.86078
[epoch: 3, batch:    420] loss: 0.02653 time model: 0.02298 acc: 0.86161
[epoch: 3, batch:    440] loss: 0.02641 time model: 0.02298 acc: 0.86278
[epoch: 3, batch:    460] loss: 0.02644 time model: 0.02296 acc: 0.86304
[epoch: 3, batch:    480] loss: 0.02645 time model: 0.02295 acc: 0.86341
[epoch: 3, batch:    500] loss: 0.02631 time model: 0.02296 acc: 0.86400
[epoch: 3, batch:    520] loss: 0.02603 time model: 0.02296 acc: 0.86418
[epoch: 3, batch:    540] loss: 0.02592 time model: 0.02296 acc: 0.86412
[epoch: 3, batch:    560] loss: 0.02579 time model: 0.02296 acc: 0.86440
[epoch: 3, batch:    580] loss: 0.02563 time model: 0.02296 acc: 0.86552
[epoch: 3, batch:    600] loss: 0.02552 time model: 0.02296 acc: 0.86604
[epoch: 3, batch:    620] loss: 0.02540 time model: 0.02297 acc: 0.86623
[epoch: 3, batch:    640] loss: 0.02517 time model: 0.02298 acc: 0.86777
[epoch: 3, batch:    660] loss: 0.02503 time model: 0.02297 acc: 0.86837
[epoch: 3, batch:    680] loss: 0.02513 time model: 0.02298 acc: 0.86792
[epoch: 3, batch:    700] loss: 0.02499 time model: 0.02299 acc: 0.86857
[epoch: 3, batch:    720] loss: 0.02508 time model: 0.02298 acc: 0.86858
[epoch: 3, batch:    740] loss: 0.02512 time model: 0.02298 acc: 0.86841
[epoch: 3, batch:    760] loss: 0.02483 time model: 0.02297 acc: 0.86974
[epoch: 3, batch:    780] loss: 0.02463 time model: 0.02297 acc: 0.87099
[epoch: 3, batch:    800] loss: 0.02461 time model: 0.02296 acc: 0.87141
[epoch: 3, batch:    820] loss: 0.02459 time model: 0.02297 acc: 0.87149
[epoch: 3, batch:    840] loss: 0.02450 time model: 0.02296 acc: 0.87195
[epoch: 3, batch:    860] loss: 0.02439 time model: 0.02296 acc: 0.87238
[epoch: 3, batch:    880] loss: 0.02439 time model: 0.02296 acc: 0.87259
[epoch: 3, batch:    900] loss: 0.02432 time model: 0.02296 acc: 0.87285
[epoch: 3, batch:    920] loss: 0.02440 time model: 0.02296 acc: 0.87249
[epoch: 3, batch:    940] loss: 0.02427 time model: 0.02296 acc: 0.87307
[epoch: 3, batch:    960] loss: 0.02411 time model: 0.02296 acc: 0.87402
[epoch: 3, batch:    980] loss: 0.02424 time model: 0.02296 acc: 0.87360
[epoch: 3, batch:   1000] loss: 0.02424 time model: 0.02297 acc: 0.87356
[epoch: 3, batch:   1020] loss: 0.02417 time model: 0.02296 acc: 0.87408
[epoch: 3, batch:   1040] loss: 0.02407 time model: 0.02297 acc: 0.87422
[epoch: 3, batch:   1060] loss: 0.02395 time model: 0.02297 acc: 0.87500
[epoch: 3, batch:   1080] loss: 0.02391 time model: 0.02297 acc: 0.87512
[epoch: 3, batch:   1100] loss: 0.02394 time model: 0.02297 acc: 0.87517
[epoch: 3, batch:   1120] loss: 0.02398 time model: 0.02297 acc: 0.87467
[epoch: 3, batch:   1140] loss: 0.02403 time model: 0.02298 acc: 0.87418
[epoch: 3, batch:   1160] loss: 0.02399 time model: 0.02298 acc: 0.87414
[epoch: 3, batch:   1180] loss: 0.02394 time model: 0.02297 acc: 0.87463
[epoch: 3, batch:   1200] loss: 0.02386 time model: 0.02298 acc: 0.87505
[epoch: 3, batch:   1220] loss: 0.02385 time model: 0.02298 acc: 0.87536
[epoch: 3, batch:   1240] loss: 0.02383 time model: 0.02297 acc: 0.87566
[epoch: 3, batch:   1260] loss: 0.02370 time model: 0.02297 acc: 0.87614
[epoch: 3, batch:   1280] loss: 0.02359 time model: 0.02297 acc: 0.87656
[epoch: 3, batch:   1300] loss: 0.02363 time model: 0.02297 acc: 0.87659
[epoch: 3, batch:   1320] loss: 0.02392 time model: 0.02296 acc: 0.87666
[epoch: 3, batch:     20] loss: 0.02489 time model: 0.01615 acc: 0.87187
[epoch: 3, batch:     40] loss: 0.02132 time model: 0.01607 acc: 0.89219
[epoch: 3, batch:     60] loss: 0.02294 time model: 0.01586 acc: 0.88854
[epoch: 3, batch:     80] loss: 0.02299 time model: 0.01589 acc: 0.88672
[epoch: 3, batch:    100] loss: 0.02334 time model: 0.01597 acc: 0.88562
[epoch: 3, batch:    120] loss: 0.02327 time model: 0.01595 acc: 0.88594
[epoch: 3, batch:    140] loss: 0.02318 time model: 0.01596 acc: 0.88527
[epoch: 3, batch:    160] loss: 0.02271 time model: 0.01591 acc: 0.88750
epoch:3 train loss: 0.023915180404712678 train acc: 0.8766642975598199 valid loss: 0.022630788871586947 valid acc: 0.8878540939076446
[epoch: 4, batch:     20] loss: 0.02423 time model: 0.02304 acc: 0.86562
[epoch: 4, batch:     40] loss: 0.02323 time model: 0.02318 acc: 0.87813
[epoch: 4, batch:     60] loss: 0.02283 time model: 0.02297 acc: 0.88333
[epoch: 4, batch:     80] loss: 0.02134 time model: 0.02294 acc: 0.89219
[epoch: 4, batch:    100] loss: 0.02055 time model: 0.02292 acc: 0.89625
[epoch: 4, batch:    120] loss: 0.02009 time model: 0.02297 acc: 0.89740
[epoch: 4, batch:    140] loss: 0.01982 time model: 0.02294 acc: 0.89687
[epoch: 4, batch:    160] loss: 0.02003 time model: 0.02292 acc: 0.89766
[epoch: 4, batch:    180] loss: 0.01974 time model: 0.02295 acc: 0.89757
[epoch: 4, batch:    200] loss: 0.02020 time model: 0.02295 acc: 0.89563
[epoch: 4, batch:    220] loss: 0.02048 time model: 0.02293 acc: 0.89375
[epoch: 4, batch:    240] loss: 0.02054 time model: 0.02295 acc: 0.89349
[epoch: 4, batch:    260] loss: 0.02008 time model: 0.02298 acc: 0.89591
[epoch: 4, batch:    280] loss: 0.01980 time model: 0.02296 acc: 0.89710
[epoch: 4, batch:    300] loss: 0.01972 time model: 0.02296 acc: 0.89646
[epoch: 4, batch:    320] loss: 0.01939 time model: 0.02296 acc: 0.89863
[epoch: 4, batch:    340] loss: 0.01987 time model: 0.02296 acc: 0.89632
[epoch: 4, batch:    360] loss: 0.01991 time model: 0.02296 acc: 0.89670
[epoch: 4, batch:    380] loss: 0.01990 time model: 0.02295 acc: 0.89720
[epoch: 4, batch:    400] loss: 0.01973 time model: 0.02295 acc: 0.89812
[epoch: 4, batch:    420] loss: 0.01978 time model: 0.02296 acc: 0.89807
[epoch: 4, batch:    440] loss: 0.01980 time model: 0.02295 acc: 0.89844
[epoch: 4, batch:    460] loss: 0.01963 time model: 0.02294 acc: 0.89905
[epoch: 4, batch:    480] loss: 0.01959 time model: 0.02293 acc: 0.89961
[epoch: 4, batch:    500] loss: 0.01947 time model: 0.02293 acc: 0.90038
[epoch: 4, batch:    520] loss: 0.01926 time model: 0.02294 acc: 0.90108
[epoch: 4, batch:    540] loss: 0.01924 time model: 0.02293 acc: 0.90127
[epoch: 4, batch:    560] loss: 0.01935 time model: 0.02293 acc: 0.90112
[epoch: 4, batch:    580] loss: 0.01922 time model: 0.02293 acc: 0.90183
[epoch: 4, batch:    600] loss: 0.01906 time model: 0.02293 acc: 0.90240
[epoch: 4, batch:    620] loss: 0.01904 time model: 0.02292 acc: 0.90262
[epoch: 4, batch:    640] loss: 0.01907 time model: 0.02291 acc: 0.90156
[epoch: 4, batch:    660] loss: 0.01906 time model: 0.02292 acc: 0.90133
[epoch: 4, batch:    680] loss: 0.01897 time model: 0.02292 acc: 0.90156
[epoch: 4, batch:    700] loss: 0.01909 time model: 0.02292 acc: 0.90134
[epoch: 4, batch:    720] loss: 0.01915 time model: 0.02292 acc: 0.90095
[epoch: 4, batch:    740] loss: 0.01902 time model: 0.02292 acc: 0.90194
[epoch: 4, batch:    760] loss: 0.01901 time model: 0.02291 acc: 0.90189
[epoch: 4, batch:    780] loss: 0.01896 time model: 0.02292 acc: 0.90248
[epoch: 4, batch:    800] loss: 0.01887 time model: 0.02292 acc: 0.90305
[epoch: 4, batch:    820] loss: 0.01882 time model: 0.02292 acc: 0.90312
[epoch: 4, batch:    840] loss: 0.01876 time model: 0.02292 acc: 0.90335
[epoch: 4, batch:    860] loss: 0.01874 time model: 0.02291 acc: 0.90363
[epoch: 4, batch:    880] loss: 0.01874 time model: 0.02291 acc: 0.90348
[epoch: 4, batch:    900] loss: 0.01862 time model: 0.02292 acc: 0.90410
[epoch: 4, batch:    920] loss: 0.01857 time model: 0.02292 acc: 0.90408
[epoch: 4, batch:    940] loss: 0.01846 time model: 0.02292 acc: 0.90485
[epoch: 4, batch:    960] loss: 0.01840 time model: 0.02291 acc: 0.90501
[epoch: 4, batch:    980] loss: 0.01849 time model: 0.02291 acc: 0.90472
[epoch: 4, batch:   1000] loss: 0.01851 time model: 0.02291 acc: 0.90475
[epoch: 4, batch:   1020] loss: 0.01849 time model: 0.02291 acc: 0.90460
[epoch: 4, batch:   1040] loss: 0.01849 time model: 0.02291 acc: 0.90445
[epoch: 4, batch:   1060] loss: 0.01846 time model: 0.02291 acc: 0.90454
[epoch: 4, batch:   1080] loss: 0.01848 time model: 0.02291 acc: 0.90463
[epoch: 4, batch:   1100] loss: 0.01838 time model: 0.02291 acc: 0.90523
[epoch: 4, batch:   1120] loss: 0.01844 time model: 0.02290 acc: 0.90502
[epoch: 4, batch:   1140] loss: 0.01844 time model: 0.02291 acc: 0.90510
[epoch: 4, batch:   1160] loss: 0.01842 time model: 0.02291 acc: 0.90528
[epoch: 4, batch:   1180] loss: 0.01841 time model: 0.02291 acc: 0.90514
[epoch: 4, batch:   1200] loss: 0.01836 time model: 0.02291 acc: 0.90568
[epoch: 4, batch:   1220] loss: 0.01826 time model: 0.02291 acc: 0.90605
[epoch: 4, batch:   1240] loss: 0.01823 time model: 0.02291 acc: 0.90605
[epoch: 4, batch:   1260] loss: 0.01825 time model: 0.02291 acc: 0.90595
[epoch: 4, batch:   1280] loss: 0.01825 time model: 0.02291 acc: 0.90601
[epoch: 4, batch:   1300] loss: 0.01820 time model: 0.02291 acc: 0.90620
[epoch: 4, batch:   1320] loss: 0.01822 time model: 0.02291 acc: 0.90652
[epoch: 4, batch:     20] loss: 0.02518 time model: 0.01635 acc: 0.88750
[epoch: 4, batch:     40] loss: 0.02304 time model: 0.01613 acc: 0.89844
[epoch: 4, batch:     60] loss: 0.02409 time model: 0.01612 acc: 0.89271
[epoch: 4, batch:     80] loss: 0.02445 time model: 0.01620 acc: 0.89062
[epoch: 4, batch:    100] loss: 0.02410 time model: 0.01617 acc: 0.89125
[epoch: 4, batch:    120] loss: 0.02523 time model: 0.01616 acc: 0.88958
[epoch: 4, batch:    140] loss: 0.02549 time model: 0.01616 acc: 0.88795
[epoch: 4, batch:    160] loss: 0.02546 time model: 0.01610 acc: 0.88516
epoch:4 train loss: 0.018223353010928624 train acc: 0.9065150438284767 valid loss: 0.025473693954560662 valid acc: 0.8847497089639115
[epoch: 5, batch:     20] loss: 0.01720 time model: 0.02308 acc: 0.91563
[epoch: 5, batch:     40] loss: 0.01554 time model: 0.02317 acc: 0.92500
[epoch: 5, batch:     60] loss: 0.01561 time model: 0.02319 acc: 0.92917
[epoch: 5, batch:     80] loss: 0.01515 time model: 0.02312 acc: 0.93203
[epoch: 5, batch:    100] loss: 0.01632 time model: 0.02313 acc: 0.92250
[epoch: 5, batch:    120] loss: 0.01600 time model: 0.02313 acc: 0.92135
[epoch: 5, batch:    140] loss: 0.01569 time model: 0.02309 acc: 0.92277
[epoch: 5, batch:    160] loss: 0.01548 time model: 0.02312 acc: 0.92383
[epoch: 5, batch:    180] loss: 0.01551 time model: 0.02312 acc: 0.92431
[epoch: 5, batch:    200] loss: 0.01566 time model: 0.02315 acc: 0.92406
[epoch: 5, batch:    220] loss: 0.01588 time model: 0.02315 acc: 0.92301
[epoch: 5, batch:    240] loss: 0.01554 time model: 0.02317 acc: 0.92344
[epoch: 5, batch:    260] loss: 0.01556 time model: 0.02313 acc: 0.92452
[epoch: 5, batch:    280] loss: 0.01540 time model: 0.02313 acc: 0.92500
[epoch: 5, batch:    300] loss: 0.01515 time model: 0.02316 acc: 0.92563
[epoch: 5, batch:    320] loss: 0.01507 time model: 0.02314 acc: 0.92559
[epoch: 5, batch:    340] loss: 0.01492 time model: 0.02315 acc: 0.92537
[epoch: 5, batch:    360] loss: 0.01495 time model: 0.02315 acc: 0.92604
[epoch: 5, batch:    380] loss: 0.01495 time model: 0.02315 acc: 0.92516
[epoch: 5, batch:    400] loss: 0.01492 time model: 0.02317 acc: 0.92469
[epoch: 5, batch:    420] loss: 0.01487 time model: 0.02315 acc: 0.92470
[epoch: 5, batch:    440] loss: 0.01473 time model: 0.02315 acc: 0.92557
[epoch: 5, batch:    460] loss: 0.01470 time model: 0.02315 acc: 0.92568
[epoch: 5, batch:    480] loss: 0.01491 time model: 0.02314 acc: 0.92383
[epoch: 5, batch:    500] loss: 0.01492 time model: 0.02313 acc: 0.92425
[epoch: 5, batch:    520] loss: 0.01505 time model: 0.02313 acc: 0.92308
[epoch: 5, batch:    540] loss: 0.01507 time model: 0.02312 acc: 0.92292
[epoch: 5, batch:    560] loss: 0.01520 time model: 0.02311 acc: 0.92232
[epoch: 5, batch:    580] loss: 0.01523 time model: 0.02310 acc: 0.92209
[epoch: 5, batch:    600] loss: 0.01519 time model: 0.02309 acc: 0.92292
[epoch: 5, batch:    620] loss: 0.01512 time model: 0.02309 acc: 0.92379
[epoch: 5, batch:    640] loss: 0.01503 time model: 0.02310 acc: 0.92471
[epoch: 5, batch:    660] loss: 0.01508 time model: 0.02309 acc: 0.92415
[epoch: 5, batch:    680] loss: 0.01519 time model: 0.02308 acc: 0.92371
[epoch: 5, batch:    700] loss: 0.01511 time model: 0.02308 acc: 0.92429
[epoch: 5, batch:    720] loss: 0.01519 time model: 0.02308 acc: 0.92361
[epoch: 5, batch:    740] loss: 0.01524 time model: 0.02307 acc: 0.92323
[epoch: 5, batch:    760] loss: 0.01528 time model: 0.02307 acc: 0.92303
[epoch: 5, batch:    780] loss: 0.01520 time model: 0.02307 acc: 0.92316
[epoch: 5, batch:    800] loss: 0.01516 time model: 0.02307 acc: 0.92312
[epoch: 5, batch:    820] loss: 0.01518 time model: 0.02306 acc: 0.92332
[epoch: 5, batch:    840] loss: 0.01512 time model: 0.02305 acc: 0.92374
[epoch: 5, batch:    860] loss: 0.01505 time model: 0.02305 acc: 0.92376
[epoch: 5, batch:    880] loss: 0.01508 time model: 0.02304 acc: 0.92372
[epoch: 5, batch:    900] loss: 0.01503 time model: 0.02304 acc: 0.92382
[epoch: 5, batch:    920] loss: 0.01507 time model: 0.02304 acc: 0.92330
[epoch: 5, batch:    940] loss: 0.01510 time model: 0.02304 acc: 0.92334
[epoch: 5, batch:    960] loss: 0.01521 time model: 0.02304 acc: 0.92298
[epoch: 5, batch:    980] loss: 0.01523 time model: 0.02304 acc: 0.92290
[epoch: 5, batch:   1000] loss: 0.01512 time model: 0.02304 acc: 0.92350
[epoch: 5, batch:   1020] loss: 0.01512 time model: 0.02303 acc: 0.92353
[epoch: 5, batch:   1040] loss: 0.01505 time model: 0.02304 acc: 0.92362
[epoch: 5, batch:   1060] loss: 0.01499 time model: 0.02304 acc: 0.92412
[epoch: 5, batch:   1080] loss: 0.01497 time model: 0.02304 acc: 0.92396
[epoch: 5, batch:   1100] loss: 0.01491 time model: 0.02304 acc: 0.92426
[epoch: 5, batch:   1120] loss: 0.01487 time model: 0.02303 acc: 0.92444
[epoch: 5, batch:   1140] loss: 0.01488 time model: 0.02304 acc: 0.92451
[epoch: 5, batch:   1160] loss: 0.01482 time model: 0.02303 acc: 0.92468
[epoch: 5, batch:   1180] loss: 0.01478 time model: 0.02303 acc: 0.92489
[epoch: 5, batch:   1200] loss: 0.01477 time model: 0.02303 acc: 0.92500
[epoch: 5, batch:   1220] loss: 0.01471 time model: 0.02302 acc: 0.92536
[epoch: 5, batch:   1240] loss: 0.01476 time model: 0.02302 acc: 0.92530
[epoch: 5, batch:   1260] loss: 0.01477 time model: 0.02302 acc: 0.92490
[epoch: 5, batch:   1280] loss: 0.01487 time model: 0.02302 acc: 0.92446
[epoch: 5, batch:   1300] loss: 0.01484 time model: 0.02302 acc: 0.92466
[epoch: 5, batch:   1320] loss: 0.01478 time model: 0.02301 acc: 0.92533
[epoch: 5, batch:     20] loss: 0.01924 time model: 0.01582 acc: 0.89375
[epoch: 5, batch:     40] loss: 0.01860 time model: 0.01585 acc: 0.90156
[epoch: 5, batch:     60] loss: 0.01778 time model: 0.01587 acc: 0.90417
[epoch: 5, batch:     80] loss: 0.01788 time model: 0.01586 acc: 0.90703
[epoch: 5, batch:    100] loss: 0.01780 time model: 0.01586 acc: 0.90500
[epoch: 5, batch:    120] loss: 0.01791 time model: 0.01587 acc: 0.90521
[epoch: 5, batch:    140] loss: 0.01905 time model: 0.01583 acc: 0.90134
[epoch: 5, batch:    160] loss: 0.01988 time model: 0.01581 acc: 0.89805
epoch:5 train loss: 0.01478263555905232 train acc: 0.9253257521914239 valid loss: 0.020002762233743754 valid acc: 0.896779200620877
[epoch: 6, batch:     20] loss: 0.01878 time model: 0.02282 acc: 0.90938
[epoch: 6, batch:     40] loss: 0.01969 time model: 0.02304 acc: 0.89687
[epoch: 6, batch:     60] loss: 0.01773 time model: 0.02290 acc: 0.90417
[epoch: 6, batch:     80] loss: 0.01538 time model: 0.02284 acc: 0.91484
[epoch: 6, batch:    100] loss: 0.01514 time model: 0.02284 acc: 0.91563
[epoch: 6, batch:    120] loss: 0.01514 time model: 0.02285 acc: 0.91927
[epoch: 6, batch:    140] loss: 0.01488 time model: 0.02285 acc: 0.92054
[epoch: 6, batch:    160] loss: 0.01446 time model: 0.02284 acc: 0.92383
[epoch: 6, batch:    180] loss: 0.01449 time model: 0.02285 acc: 0.92431
[epoch: 6, batch:    200] loss: 0.01467 time model: 0.02285 acc: 0.92344
[epoch: 6, batch:    220] loss: 0.01435 time model: 0.02285 acc: 0.92528
[epoch: 6, batch:    240] loss: 0.01470 time model: 0.02285 acc: 0.92526
[epoch: 6, batch:    260] loss: 0.01485 time model: 0.02284 acc: 0.92476
[epoch: 6, batch:    280] loss: 0.01476 time model: 0.02286 acc: 0.92522
[epoch: 6, batch:    300] loss: 0.01474 time model: 0.02287 acc: 0.92625
[epoch: 6, batch:    320] loss: 0.01484 time model: 0.02285 acc: 0.92539
[epoch: 6, batch:    340] loss: 0.01469 time model: 0.02284 acc: 0.92592
[epoch: 6, batch:    360] loss: 0.01465 time model: 0.02284 acc: 0.92604
[epoch: 6, batch:    380] loss: 0.01450 time model: 0.02284 acc: 0.92681
[epoch: 6, batch:    400] loss: 0.01432 time model: 0.02284 acc: 0.92781
[epoch: 6, batch:    420] loss: 0.01401 time model: 0.02285 acc: 0.92976
[epoch: 6, batch:    440] loss: 0.01397 time model: 0.02285 acc: 0.92997
[epoch: 6, batch:    460] loss: 0.01367 time model: 0.02284 acc: 0.93125
[epoch: 6, batch:    480] loss: 0.01352 time model: 0.02285 acc: 0.93190
[epoch: 6, batch:    500] loss: 0.01343 time model: 0.02284 acc: 0.93200
[epoch: 6, batch:    520] loss: 0.01336 time model: 0.02286 acc: 0.93209
[epoch: 6, batch:    540] loss: 0.01326 time model: 0.02288 acc: 0.93275
[epoch: 6, batch:    560] loss: 0.01315 time model: 0.02288 acc: 0.93304
[epoch: 6, batch:    580] loss: 0.01304 time model: 0.02288 acc: 0.93351
[epoch: 6, batch:    600] loss: 0.01312 time model: 0.02288 acc: 0.93323
[epoch: 6, batch:    620] loss: 0.01314 time model: 0.02289 acc: 0.93317
[epoch: 6, batch:    640] loss: 0.01334 time model: 0.02288 acc: 0.93281
[epoch: 6, batch:    660] loss: 0.01317 time model: 0.02288 acc: 0.93362
[epoch: 6, batch:    680] loss: 0.01318 time model: 0.02288 acc: 0.93382
[epoch: 6, batch:    700] loss: 0.01317 time model: 0.02288 acc: 0.93357
[epoch: 6, batch:    720] loss: 0.01311 time model: 0.02289 acc: 0.93351
[epoch: 6, batch:    740] loss: 0.01309 time model: 0.02289 acc: 0.93353
[epoch: 6, batch:    760] loss: 0.01313 time model: 0.02288 acc: 0.93339
[epoch: 6, batch:    780] loss: 0.01301 time model: 0.02288 acc: 0.93397
[epoch: 6, batch:    800] loss: 0.01317 time model: 0.02288 acc: 0.93328
[epoch: 6, batch:    820] loss: 0.01325 time model: 0.02289 acc: 0.93239
[epoch: 6, batch:    840] loss: 0.01334 time model: 0.02289 acc: 0.93170
[epoch: 6, batch:    860] loss: 0.01338 time model: 0.02289 acc: 0.93169
[epoch: 6, batch:    880] loss: 0.01335 time model: 0.02289 acc: 0.93196
[epoch: 6, batch:    900] loss: 0.01328 time model: 0.02290 acc: 0.93236
[epoch: 6, batch:    920] loss: 0.01327 time model: 0.02290 acc: 0.93281
[epoch: 6, batch:    940] loss: 0.01325 time model: 0.02290 acc: 0.93265
[epoch: 6, batch:    960] loss: 0.01329 time model: 0.02290 acc: 0.93262
[epoch: 6, batch:    980] loss: 0.01328 time model: 0.02290 acc: 0.93240
[epoch: 6, batch:   1000] loss: 0.01321 time model: 0.02289 acc: 0.93263
[epoch: 6, batch:   1020] loss: 0.01324 time model: 0.02289 acc: 0.93241
[epoch: 6, batch:   1040] loss: 0.01319 time model: 0.02289 acc: 0.93263
[epoch: 6, batch:   1060] loss: 0.01318 time model: 0.02288 acc: 0.93261
[epoch: 6, batch:   1080] loss: 0.01313 time model: 0.02288 acc: 0.93270
[epoch: 6, batch:   1100] loss: 0.01313 time model: 0.02288 acc: 0.93284
[epoch: 6, batch:   1120] loss: 0.01312 time model: 0.02288 acc: 0.93304
[epoch: 6, batch:   1140] loss: 0.01312 time model: 0.02289 acc: 0.93289
[epoch: 6, batch:   1160] loss: 0.01309 time model: 0.02289 acc: 0.93297
[epoch: 6, batch:   1180] loss: 0.01305 time model: 0.02289 acc: 0.93294
[epoch: 6, batch:   1200] loss: 0.01310 time model: 0.02289 acc: 0.93281
[epoch: 6, batch:   1220] loss: 0.01306 time model: 0.02290 acc: 0.93304
[epoch: 6, batch:   1240] loss: 0.01317 time model: 0.02290 acc: 0.93251
[epoch: 6, batch:   1260] loss: 0.01314 time model: 0.02289 acc: 0.93269
[epoch: 6, batch:   1280] loss: 0.01309 time model: 0.02289 acc: 0.93291
[epoch: 6, batch:   1300] loss: 0.01308 time model: 0.02290 acc: 0.93298
[epoch: 6, batch:   1320] loss: 0.01322 time model: 0.02288 acc: 0.93267
[epoch: 6, batch:     20] loss: 0.01519 time model: 0.01619 acc: 0.93437
[epoch: 6, batch:     40] loss: 0.01778 time model: 0.01599 acc: 0.91875
[epoch: 6, batch:     60] loss: 0.01874 time model: 0.01603 acc: 0.91563
[epoch: 6, batch:     80] loss: 0.01798 time model: 0.01601 acc: 0.91719
[epoch: 6, batch:    100] loss: 0.01777 time model: 0.01598 acc: 0.91812
[epoch: 6, batch:    120] loss: 0.01811 time model: 0.01603 acc: 0.91406
[epoch: 6, batch:    140] loss: 0.01797 time model: 0.01602 acc: 0.91563
[epoch: 6, batch:    160] loss: 0.01718 time model: 0.01600 acc: 0.91953
epoch:6 train loss: 0.01322032982874922 train acc: 0.9326699834162521 valid loss: 0.017099555560124213 valid acc: 0.9200620876988747
[epoch: 7, batch:     20] loss: 0.01167 time model: 0.02277 acc: 0.94688
[epoch: 7, batch:     40] loss: 0.00960 time model: 0.02274 acc: 0.95156
[epoch: 7, batch:     60] loss: 0.00951 time model: 0.02286 acc: 0.95000
[epoch: 7, batch:     80] loss: 0.00999 time model: 0.02275 acc: 0.94766
[epoch: 7, batch:    100] loss: 0.00985 time model: 0.02277 acc: 0.94812
[epoch: 7, batch:    120] loss: 0.00939 time model: 0.02280 acc: 0.95104
[epoch: 7, batch:    140] loss: 0.01022 time model: 0.02279 acc: 0.94777
[epoch: 7, batch:    160] loss: 0.01026 time model: 0.02281 acc: 0.94727
[epoch: 7, batch:    180] loss: 0.01057 time model: 0.02285 acc: 0.94583
[epoch: 7, batch:    200] loss: 0.01108 time model: 0.02282 acc: 0.94406
[epoch: 7, batch:    220] loss: 0.01103 time model: 0.02282 acc: 0.94347
[epoch: 7, batch:    240] loss: 0.01065 time model: 0.02282 acc: 0.94609
[epoch: 7, batch:    260] loss: 0.01041 time model: 0.02283 acc: 0.94760
[epoch: 7, batch:    280] loss: 0.01041 time model: 0.02285 acc: 0.94821
[epoch: 7, batch:    300] loss: 0.01021 time model: 0.02284 acc: 0.94812
[epoch: 7, batch:    320] loss: 0.01014 time model: 0.02285 acc: 0.94824
[epoch: 7, batch:    340] loss: 0.01026 time model: 0.02287 acc: 0.94798
[epoch: 7, batch:    360] loss: 0.01045 time model: 0.02288 acc: 0.94670
[epoch: 7, batch:    380] loss: 0.01078 time model: 0.02290 acc: 0.94539
[epoch: 7, batch:    400] loss: 0.01080 time model: 0.02288 acc: 0.94547
[epoch: 7, batch:    420] loss: 0.01070 time model: 0.02289 acc: 0.94583
[epoch: 7, batch:    440] loss: 0.01054 time model: 0.02290 acc: 0.94673
[epoch: 7, batch:    460] loss: 0.01054 time model: 0.02291 acc: 0.94674
[epoch: 7, batch:    480] loss: 0.01051 time model: 0.02290 acc: 0.94688
[epoch: 7, batch:    500] loss: 0.01057 time model: 0.02289 acc: 0.94625
[epoch: 7, batch:    520] loss: 0.01056 time model: 0.02288 acc: 0.94639
[epoch: 7, batch:    540] loss: 0.01069 time model: 0.02289 acc: 0.94560
[epoch: 7, batch:    560] loss: 0.01097 time model: 0.02288 acc: 0.94420
[epoch: 7, batch:    580] loss: 0.01105 time model: 0.02288 acc: 0.94375
[epoch: 7, batch:    600] loss: 0.01101 time model: 0.02288 acc: 0.94396
[epoch: 7, batch:    620] loss: 0.01116 time model: 0.02290 acc: 0.94335
[epoch: 7, batch:    640] loss: 0.01112 time model: 0.02290 acc: 0.94375
[epoch: 7, batch:    660] loss: 0.01112 time model: 0.02291 acc: 0.94375
[epoch: 7, batch:    680] loss: 0.01110 time model: 0.02290 acc: 0.94421
[epoch: 7, batch:    700] loss: 0.01111 time model: 0.02291 acc: 0.94393
[epoch: 7, batch:    720] loss: 0.01114 time model: 0.02291 acc: 0.94358
[epoch: 7, batch:    740] loss: 0.01112 time model: 0.02291 acc: 0.94383
[epoch: 7, batch:    760] loss: 0.01109 time model: 0.02291 acc: 0.94400
[epoch: 7, batch:    780] loss: 0.01110 time model: 0.02291 acc: 0.94431
[epoch: 7, batch:    800] loss: 0.01117 time model: 0.02291 acc: 0.94375
[epoch: 7, batch:    820] loss: 0.01124 time model: 0.02292 acc: 0.94337
[epoch: 7, batch:    840] loss: 0.01139 time model: 0.02292 acc: 0.94293
[epoch: 7, batch:    860] loss: 0.01140 time model: 0.02292 acc: 0.94317
[epoch: 7, batch:    880] loss: 0.01140 time model: 0.02291 acc: 0.94297
[epoch: 7, batch:    900] loss: 0.01137 time model: 0.02292 acc: 0.94333
[epoch: 7, batch:    920] loss: 0.01132 time model: 0.02292 acc: 0.94314
[epoch: 7, batch:    940] loss: 0.01132 time model: 0.02292 acc: 0.94315
[epoch: 7, batch:    960] loss: 0.01136 time model: 0.02293 acc: 0.94277
[epoch: 7, batch:    980] loss: 0.01146 time model: 0.02292 acc: 0.94184
[epoch: 7, batch:   1000] loss: 0.01142 time model: 0.02292 acc: 0.94200
[epoch: 7, batch:   1020] loss: 0.01135 time model: 0.02293 acc: 0.94228
[epoch: 7, batch:   1040] loss: 0.01133 time model: 0.02292 acc: 0.94225
[epoch: 7, batch:   1060] loss: 0.01135 time model: 0.02292 acc: 0.94210
[epoch: 7, batch:   1080] loss: 0.01130 time model: 0.02293 acc: 0.94230
[epoch: 7, batch:   1100] loss: 0.01123 time model: 0.02293 acc: 0.94261
[epoch: 7, batch:   1120] loss: 0.01131 time model: 0.02293 acc: 0.94235
[epoch: 7, batch:   1140] loss: 0.01138 time model: 0.02294 acc: 0.94200
[epoch: 7, batch:   1160] loss: 0.01138 time model: 0.02294 acc: 0.94186
[epoch: 7, batch:   1180] loss: 0.01136 time model: 0.02294 acc: 0.94179
[epoch: 7, batch:   1200] loss: 0.01137 time model: 0.02293 acc: 0.94172
[epoch: 7, batch:   1220] loss: 0.01135 time model: 0.02293 acc: 0.94165
[epoch: 7, batch:   1240] loss: 0.01139 time model: 0.02293 acc: 0.94153
[epoch: 7, batch:   1260] loss: 0.01134 time model: 0.02293 acc: 0.94182
[epoch: 7, batch:   1280] loss: 0.01127 time model: 0.02293 acc: 0.94214
[epoch: 7, batch:   1300] loss: 0.01123 time model: 0.02293 acc: 0.94231
[epoch: 7, batch:   1320] loss: 0.01121 time model: 0.02292 acc: 0.94238
[epoch: 7, batch:     20] loss: 0.01689 time model: 0.01605 acc: 0.93750
[epoch: 7, batch:     40] loss: 0.01832 time model: 0.01598 acc: 0.92812
[epoch: 7, batch:     60] loss: 0.01824 time model: 0.01601 acc: 0.92188
[epoch: 7, batch:     80] loss: 0.01884 time model: 0.01604 acc: 0.92109
[epoch: 7, batch:    100] loss: 0.01830 time model: 0.01602 acc: 0.92063
[epoch: 7, batch:    120] loss: 0.02048 time model: 0.01603 acc: 0.91406
[epoch: 7, batch:    140] loss: 0.02080 time model: 0.01604 acc: 0.91429
[epoch: 7, batch:    160] loss: 0.02005 time model: 0.01598 acc: 0.91641
epoch:7 train loss: 0.011214183212329188 train acc: 0.9423833214877991 valid loss: 0.01997762903149016 valid acc: 0.916569654637175
[epoch: 8, batch:     20] loss: 0.01014 time model: 0.02301 acc: 0.95000
[epoch: 8, batch:     40] loss: 0.00885 time model: 0.02302 acc: 0.95156
[epoch: 8, batch:     60] loss: 0.00971 time model: 0.02294 acc: 0.94792
[epoch: 8, batch:     80] loss: 0.00893 time model: 0.02297 acc: 0.95156
[epoch: 8, batch:    100] loss: 0.00840 time model: 0.02300 acc: 0.95375
[epoch: 8, batch:    120] loss: 0.00820 time model: 0.02307 acc: 0.95677
[epoch: 8, batch:    140] loss: 0.00837 time model: 0.02306 acc: 0.95670
[epoch: 8, batch:    160] loss: 0.00943 time model: 0.02302 acc: 0.95273
[epoch: 8, batch:    180] loss: 0.00941 time model: 0.02302 acc: 0.95347
[epoch: 8, batch:    200] loss: 0.00975 time model: 0.02298 acc: 0.95094
[epoch: 8, batch:    220] loss: 0.00987 time model: 0.02298 acc: 0.95000
[epoch: 8, batch:    240] loss: 0.00964 time model: 0.02297 acc: 0.95156
[epoch: 8, batch:    260] loss: 0.00981 time model: 0.02296 acc: 0.95024
[epoch: 8, batch:    280] loss: 0.00969 time model: 0.02297 acc: 0.95000
[epoch: 8, batch:    300] loss: 0.00940 time model: 0.02295 acc: 0.95167
[epoch: 8, batch:    320] loss: 0.00919 time model: 0.02294 acc: 0.95215
[epoch: 8, batch:    340] loss: 0.00907 time model: 0.02293 acc: 0.95294
[epoch: 8, batch:    360] loss: 0.00896 time model: 0.02293 acc: 0.95330
[epoch: 8, batch:    380] loss: 0.00913 time model: 0.02292 acc: 0.95263
[epoch: 8, batch:    400] loss: 0.00919 time model: 0.02292 acc: 0.95281
[epoch: 8, batch:    420] loss: 0.00924 time model: 0.02292 acc: 0.95283
[epoch: 8, batch:    440] loss: 0.00932 time model: 0.02292 acc: 0.95199
[epoch: 8, batch:    460] loss: 0.00944 time model: 0.02292 acc: 0.95190
[epoch: 8, batch:    480] loss: 0.00950 time model: 0.02291 acc: 0.95195
[epoch: 8, batch:    500] loss: 0.00960 time model: 0.02292 acc: 0.95150
[epoch: 8, batch:    520] loss: 0.00965 time model: 0.02292 acc: 0.95072
[epoch: 8, batch:    540] loss: 0.00964 time model: 0.02293 acc: 0.95069
[epoch: 8, batch:    560] loss: 0.00959 time model: 0.02292 acc: 0.95112
[epoch: 8, batch:    580] loss: 0.00957 time model: 0.02291 acc: 0.95086
[epoch: 8, batch:    600] loss: 0.00943 time model: 0.02291 acc: 0.95146
[epoch: 8, batch:    620] loss: 0.00942 time model: 0.02290 acc: 0.95121
[epoch: 8, batch:    640] loss: 0.00934 time model: 0.02289 acc: 0.95186
[epoch: 8, batch:    660] loss: 0.00938 time model: 0.02290 acc: 0.95189
[epoch: 8, batch:    680] loss: 0.00933 time model: 0.02290 acc: 0.95230
[epoch: 8, batch:    700] loss: 0.00929 time model: 0.02290 acc: 0.95241
[epoch: 8, batch:    720] loss: 0.00933 time model: 0.02290 acc: 0.95182
[epoch: 8, batch:    740] loss: 0.00951 time model: 0.02289 acc: 0.95059
[epoch: 8, batch:    760] loss: 0.00977 time model: 0.02289 acc: 0.94967
[epoch: 8, batch:    780] loss: 0.00985 time model: 0.02289 acc: 0.94928
[epoch: 8, batch:    800] loss: 0.00977 time model: 0.02288 acc: 0.94984
[epoch: 8, batch:    820] loss: 0.00970 time model: 0.02288 acc: 0.95023
[epoch: 8, batch:    840] loss: 0.00967 time model: 0.02287 acc: 0.95030
[epoch: 8, batch:    860] loss: 0.00964 time model: 0.02287 acc: 0.95007
[epoch: 8, batch:    880] loss: 0.00962 time model: 0.02288 acc: 0.95014
[epoch: 8, batch:    900] loss: 0.00968 time model: 0.02287 acc: 0.94958
[epoch: 8, batch:    920] loss: 0.00978 time model: 0.02288 acc: 0.94891
[epoch: 8, batch:    940] loss: 0.00978 time model: 0.02288 acc: 0.94920
[epoch: 8, batch:    960] loss: 0.00976 time model: 0.02288 acc: 0.94948
[epoch: 8, batch:    980] loss: 0.00976 time model: 0.02288 acc: 0.94955
[epoch: 8, batch:   1000] loss: 0.00984 time model: 0.02288 acc: 0.94950
[epoch: 8, batch:   1020] loss: 0.00978 time model: 0.02288 acc: 0.94957
[epoch: 8, batch:   1040] loss: 0.00984 time model: 0.02289 acc: 0.94934
[epoch: 8, batch:   1060] loss: 0.00981 time model: 0.02289 acc: 0.94953
[epoch: 8, batch:   1080] loss: 0.00973 time model: 0.02288 acc: 0.94988
[epoch: 8, batch:   1100] loss: 0.00970 time model: 0.02289 acc: 0.94994
[epoch: 8, batch:   1120] loss: 0.00990 time model: 0.02288 acc: 0.94894
[epoch: 8, batch:   1140] loss: 0.00992 time model: 0.02289 acc: 0.94868
[epoch: 8, batch:   1160] loss: 0.00998 time model: 0.02288 acc: 0.94865
[epoch: 8, batch:   1180] loss: 0.00996 time model: 0.02288 acc: 0.94846
[epoch: 8, batch:   1200] loss: 0.00993 time model: 0.02288 acc: 0.94865
[epoch: 8, batch:   1220] loss: 0.00992 time model: 0.02289 acc: 0.94867
[epoch: 8, batch:   1240] loss: 0.00989 time model: 0.02290 acc: 0.94879
[epoch: 8, batch:   1260] loss: 0.00989 time model: 0.02290 acc: 0.94891
[epoch: 8, batch:   1280] loss: 0.00986 time model: 0.02290 acc: 0.94907
[epoch: 8, batch:   1300] loss: 0.00989 time model: 0.02290 acc: 0.94889
[epoch: 8, batch:   1320] loss: 0.00986 time model: 0.02288 acc: 0.94902
[epoch: 8, batch:     20] loss: 0.01116 time model: 0.01584 acc: 0.95625
[epoch: 8, batch:     40] loss: 0.01192 time model: 0.01598 acc: 0.94219
[epoch: 8, batch:     60] loss: 0.01383 time model: 0.01596 acc: 0.93542
[epoch: 8, batch:     80] loss: 0.01585 time model: 0.01601 acc: 0.92812
[epoch: 8, batch:    100] loss: 0.01593 time model: 0.01596 acc: 0.92437
[epoch: 8, batch:    120] loss: 0.01623 time model: 0.01598 acc: 0.92292
[epoch: 8, batch:    140] loss: 0.01671 time model: 0.01594 acc: 0.92054
[epoch: 8, batch:    160] loss: 0.01742 time model: 0.01591 acc: 0.91758
epoch:8 train loss: 0.009863369495869813 train acc: 0.9490168206586117 valid loss: 0.017357880390564694 valid acc: 0.9177337989910749
[epoch: 9, batch:     20] loss: 0.00983 time model: 0.02323 acc: 0.94375
[epoch: 9, batch:     40] loss: 0.00863 time model: 0.02322 acc: 0.95312
[epoch: 9, batch:     60] loss: 0.00862 time model: 0.02314 acc: 0.95104
[epoch: 9, batch:     80] loss: 0.00828 time model: 0.02305 acc: 0.94922
[epoch: 9, batch:    100] loss: 0.00881 time model: 0.02303 acc: 0.94625
[epoch: 9, batch:    120] loss: 0.00942 time model: 0.02301 acc: 0.94375
[epoch: 9, batch:    140] loss: 0.00912 time model: 0.02297 acc: 0.94688
[epoch: 9, batch:    160] loss: 0.00889 time model: 0.02295 acc: 0.94805
[epoch: 9, batch:    180] loss: 0.00881 time model: 0.02294 acc: 0.94896
[epoch: 9, batch:    200] loss: 0.00899 time model: 0.02293 acc: 0.94844
[epoch: 9, batch:    220] loss: 0.00914 time model: 0.02293 acc: 0.94801
[epoch: 9, batch:    240] loss: 0.00900 time model: 0.02290 acc: 0.94948
[epoch: 9, batch:    260] loss: 0.00883 time model: 0.02292 acc: 0.95048
[epoch: 9, batch:    280] loss: 0.00896 time model: 0.02293 acc: 0.94933
[epoch: 9, batch:    300] loss: 0.00872 time model: 0.02292 acc: 0.95063
[epoch: 9, batch:    320] loss: 0.00878 time model: 0.02292 acc: 0.95059
[epoch: 9, batch:    340] loss: 0.00876 time model: 0.02291 acc: 0.95110
[epoch: 9, batch:    360] loss: 0.00882 time model: 0.02290 acc: 0.95139
[epoch: 9, batch:    380] loss: 0.00911 time model: 0.02291 acc: 0.94984
[epoch: 9, batch:    400] loss: 0.00894 time model: 0.02292 acc: 0.95078
[epoch: 9, batch:    420] loss: 0.00882 time model: 0.02293 acc: 0.95149
[epoch: 9, batch:    440] loss: 0.00863 time model: 0.02292 acc: 0.95227
[epoch: 9, batch:    460] loss: 0.00847 time model: 0.02291 acc: 0.95299
[epoch: 9, batch:    480] loss: 0.00832 time model: 0.02291 acc: 0.95378
[epoch: 9, batch:    500] loss: 0.00852 time model: 0.02291 acc: 0.95275
[epoch: 9, batch:    520] loss: 0.00842 time model: 0.02290 acc: 0.95325
[epoch: 9, batch:    540] loss: 0.00838 time model: 0.02290 acc: 0.95347
[epoch: 9, batch:    560] loss: 0.00846 time model: 0.02290 acc: 0.95279
[epoch: 9, batch:    580] loss: 0.00844 time model: 0.02290 acc: 0.95269
[epoch: 9, batch:    600] loss: 0.00866 time model: 0.02289 acc: 0.95146
[epoch: 9, batch:    620] loss: 0.00865 time model: 0.02289 acc: 0.95121
[epoch: 9, batch:    640] loss: 0.00863 time model: 0.02289 acc: 0.95127
[epoch: 9, batch:    660] loss: 0.00854 time model: 0.02290 acc: 0.95170
[epoch: 9, batch:    680] loss: 0.00858 time model: 0.02290 acc: 0.95184
[epoch: 9, batch:    700] loss: 0.00854 time model: 0.02290 acc: 0.95214
[epoch: 9, batch:    720] loss: 0.00851 time model: 0.02290 acc: 0.95234
[epoch: 9, batch:    740] loss: 0.00856 time model: 0.02290 acc: 0.95203
[epoch: 9, batch:    760] loss: 0.00859 time model: 0.02290 acc: 0.95189
[epoch: 9, batch:    780] loss: 0.00855 time model: 0.02290 acc: 0.95224
[epoch: 9, batch:    800] loss: 0.00852 time model: 0.02290 acc: 0.95258
[epoch: 9, batch:    820] loss: 0.00848 time model: 0.02290 acc: 0.95282
[epoch: 9, batch:    840] loss: 0.00851 time model: 0.02289 acc: 0.95268
[epoch: 9, batch:    860] loss: 0.00844 time model: 0.02290 acc: 0.95305
[epoch: 9, batch:    880] loss: 0.00843 time model: 0.02290 acc: 0.95341
[epoch: 9, batch:    900] loss: 0.00835 time model: 0.02290 acc: 0.95375
[epoch: 9, batch:    920] loss: 0.00833 time model: 0.02290 acc: 0.95408
[epoch: 9, batch:    940] loss: 0.00831 time model: 0.02289 acc: 0.95406
[epoch: 9, batch:    960] loss: 0.00827 time model: 0.02289 acc: 0.95423
[epoch: 9, batch:    980] loss: 0.00832 time model: 0.02289 acc: 0.95415
[epoch: 9, batch:   1000] loss: 0.00823 time model: 0.02290 acc: 0.95469
[epoch: 9, batch:   1020] loss: 0.00822 time model: 0.02290 acc: 0.95472
[epoch: 9, batch:   1040] loss: 0.00820 time model: 0.02290 acc: 0.95469
[epoch: 9, batch:   1060] loss: 0.00816 time model: 0.02289 acc: 0.95495
[epoch: 9, batch:   1080] loss: 0.00814 time model: 0.02289 acc: 0.95498
[epoch: 9, batch:   1100] loss: 0.00808 time model: 0.02290 acc: 0.95511
[epoch: 9, batch:   1120] loss: 0.00811 time model: 0.02290 acc: 0.95525
[epoch: 9, batch:   1140] loss: 0.00808 time model: 0.02290 acc: 0.95543
[epoch: 9, batch:   1160] loss: 0.00807 time model: 0.02291 acc: 0.95539
[epoch: 9, batch:   1180] loss: 0.00801 time model: 0.02292 acc: 0.95567
[epoch: 9, batch:   1200] loss: 0.00798 time model: 0.02292 acc: 0.95589
[epoch: 9, batch:   1220] loss: 0.00796 time model: 0.02292 acc: 0.95599
[epoch: 9, batch:   1240] loss: 0.00797 time model: 0.02292 acc: 0.95580
[epoch: 9, batch:   1260] loss: 0.00798 time model: 0.02292 acc: 0.95585
[epoch: 9, batch:   1280] loss: 0.00795 time model: 0.02292 acc: 0.95610
[epoch: 9, batch:   1300] loss: 0.00791 time model: 0.02291 acc: 0.95644
[epoch: 9, batch:   1320] loss: 0.00797 time model: 0.02291 acc: 0.95627
[epoch: 9, batch:     20] loss: 0.01552 time model: 0.01575 acc: 0.92812
[epoch: 9, batch:     40] loss: 0.01370 time model: 0.01599 acc: 0.93125
[epoch: 9, batch:     60] loss: 0.01556 time model: 0.01592 acc: 0.92396
[epoch: 9, batch:     80] loss: 0.01524 time model: 0.01585 acc: 0.92109
[epoch: 9, batch:    100] loss: 0.01556 time model: 0.01583 acc: 0.92250
[epoch: 9, batch:    120] loss: 0.01577 time model: 0.01581 acc: 0.92344
[epoch: 9, batch:    140] loss: 0.01619 time model: 0.01580 acc: 0.92411
[epoch: 9, batch:    160] loss: 0.01628 time model: 0.01578 acc: 0.92578
epoch:9 train loss: 0.007971884854344417 train acc: 0.9562662876095712 valid loss: 0.016252464323160395 valid acc: 0.9258828094683741
[epoch: 10, batch:     20] loss: 0.00445 time model: 0.02273 acc: 0.97813
[epoch: 10, batch:     40] loss: 0.00545 time model: 0.02297 acc: 0.97656
[epoch: 10, batch:     60] loss: 0.00543 time model: 0.02297 acc: 0.97292
[epoch: 10, batch:     80] loss: 0.00680 time model: 0.02292 acc: 0.96719
[epoch: 10, batch:    100] loss: 0.00812 time model: 0.02292 acc: 0.95937
[epoch: 10, batch:    120] loss: 0.00821 time model: 0.02296 acc: 0.95781
[epoch: 10, batch:    140] loss: 0.00797 time model: 0.02292 acc: 0.96071
[epoch: 10, batch:    160] loss: 0.00813 time model: 0.02294 acc: 0.95898
[epoch: 10, batch:    180] loss: 0.00824 time model: 0.02293 acc: 0.95694
[epoch: 10, batch:    200] loss: 0.00801 time model: 0.02294 acc: 0.95750
[epoch: 10, batch:    220] loss: 0.00824 time model: 0.02295 acc: 0.95682
[epoch: 10, batch:    240] loss: 0.00821 time model: 0.02292 acc: 0.95781
[epoch: 10, batch:    260] loss: 0.00803 time model: 0.02294 acc: 0.95817
[epoch: 10, batch:    280] loss: 0.00832 time model: 0.02293 acc: 0.95714
[epoch: 10, batch:    300] loss: 0.00824 time model: 0.02291 acc: 0.95792
[epoch: 10, batch:    320] loss: 0.00819 time model: 0.02288 acc: 0.95762
[epoch: 10, batch:    340] loss: 0.00813 time model: 0.02288 acc: 0.95809
[epoch: 10, batch:    360] loss: 0.00807 time model: 0.02286 acc: 0.95833
[epoch: 10, batch:    380] loss: 0.00809 time model: 0.02285 acc: 0.95822
[epoch: 10, batch:    400] loss: 0.00821 time model: 0.02286 acc: 0.95703
[epoch: 10, batch:    420] loss: 0.00821 time model: 0.02286 acc: 0.95685
[epoch: 10, batch:    440] loss: 0.00836 time model: 0.02286 acc: 0.95625
[epoch: 10, batch:    460] loss: 0.00822 time model: 0.02287 acc: 0.95720
[epoch: 10, batch:    480] loss: 0.00821 time model: 0.02285 acc: 0.95703
[epoch: 10, batch:    500] loss: 0.00822 time model: 0.02286 acc: 0.95700
[epoch: 10, batch:    520] loss: 0.00831 time model: 0.02286 acc: 0.95649
[epoch: 10, batch:    540] loss: 0.00831 time model: 0.02286 acc: 0.95625
[epoch: 10, batch:    560] loss: 0.00837 time model: 0.02287 acc: 0.95625
[epoch: 10, batch:    580] loss: 0.00831 time model: 0.02287 acc: 0.95657
[epoch: 10, batch:    600] loss: 0.00821 time model: 0.02287 acc: 0.95719
[epoch: 10, batch:    620] loss: 0.00822 time model: 0.02287 acc: 0.95716
[epoch: 10, batch:    640] loss: 0.00829 time model: 0.02286 acc: 0.95674
[epoch: 10, batch:    660] loss: 0.00830 time model: 0.02286 acc: 0.95672
[epoch: 10, batch:    680] loss: 0.00829 time model: 0.02286 acc: 0.95671
[epoch: 10, batch:    700] loss: 0.00826 time model: 0.02285 acc: 0.95679
[epoch: 10, batch:    720] loss: 0.00835 time model: 0.02284 acc: 0.95625
[epoch: 10, batch:    740] loss: 0.00823 time model: 0.02284 acc: 0.95667
[epoch: 10, batch:    760] loss: 0.00825 time model: 0.02283 acc: 0.95650
[epoch: 10, batch:    780] loss: 0.00820 time model: 0.02284 acc: 0.95665
[epoch: 10, batch:    800] loss: 0.00823 time model: 0.02284 acc: 0.95633
[epoch: 10, batch:    820] loss: 0.00821 time model: 0.02283 acc: 0.95648
[epoch: 10, batch:    840] loss: 0.00817 time model: 0.02284 acc: 0.95655
[epoch: 10, batch:    860] loss: 0.00817 time model: 0.02284 acc: 0.95669
[epoch: 10, batch:    880] loss: 0.00813 time model: 0.02283 acc: 0.95689
[epoch: 10, batch:    900] loss: 0.00804 time model: 0.02283 acc: 0.95729
[epoch: 10, batch:    920] loss: 0.00794 time model: 0.02283 acc: 0.95781
[epoch: 10, batch:    940] loss: 0.00791 time model: 0.02283 acc: 0.95811
[epoch: 10, batch:    960] loss: 0.00795 time model: 0.02282 acc: 0.95788
[epoch: 10, batch:    980] loss: 0.00806 time model: 0.02283 acc: 0.95765
[epoch: 10, batch:   1000] loss: 0.00805 time model: 0.02283 acc: 0.95775
[epoch: 10, batch:   1020] loss: 0.00806 time model: 0.02282 acc: 0.95754
[epoch: 10, batch:   1040] loss: 0.00805 time model: 0.02282 acc: 0.95769
[epoch: 10, batch:   1060] loss: 0.00804 time model: 0.02282 acc: 0.95767
[epoch: 10, batch:   1080] loss: 0.00802 time model: 0.02282 acc: 0.95758
[epoch: 10, batch:   1100] loss: 0.00802 time model: 0.02282 acc: 0.95750
[epoch: 10, batch:   1120] loss: 0.00803 time model: 0.02281 acc: 0.95770
[epoch: 10, batch:   1140] loss: 0.00797 time model: 0.02281 acc: 0.95811
[epoch: 10, batch:   1160] loss: 0.00799 time model: 0.02281 acc: 0.95819
[epoch: 10, batch:   1180] loss: 0.00796 time model: 0.02281 acc: 0.95826
[epoch: 10, batch:   1200] loss: 0.00794 time model: 0.02281 acc: 0.95828
[epoch: 10, batch:   1220] loss: 0.00798 time model: 0.02280 acc: 0.95799
[epoch: 10, batch:   1240] loss: 0.00798 time model: 0.02281 acc: 0.95781
[epoch: 10, batch:   1260] loss: 0.00796 time model: 0.02281 acc: 0.95789
[epoch: 10, batch:   1280] loss: 0.00792 time model: 0.02281 acc: 0.95806
[epoch: 10, batch:   1300] loss: 0.00789 time model: 0.02281 acc: 0.95813
[epoch: 10, batch:   1320] loss: 0.00805 time model: 0.02280 acc: 0.95811
[epoch: 10, batch:     20] loss: 0.03007 time model: 0.01593 acc: 0.89375
[epoch: 10, batch:     40] loss: 0.02162 time model: 0.01588 acc: 0.91250
[epoch: 10, batch:     60] loss: 0.02054 time model: 0.01591 acc: 0.91354
[epoch: 10, batch:     80] loss: 0.02344 time model: 0.01592 acc: 0.90547
[epoch: 10, batch:    100] loss: 0.02435 time model: 0.01587 acc: 0.90125
[epoch: 10, batch:    120] loss: 0.02526 time model: 0.01587 acc: 0.89687
[epoch: 10, batch:    140] loss: 0.02521 time model: 0.01588 acc: 0.89554
[epoch: 10, batch:    160] loss: 0.02621 time model: 0.01585 acc: 0.89258
epoch:10 train loss: 0.008052765606714776 train acc: 0.9581141909500118 valid loss: 0.02606301808489077 valid acc: 0.8932867675591774
