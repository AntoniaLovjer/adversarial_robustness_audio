[epoch: 1, batch:     20] loss: 0.17859 time model: 0.24004 acc: 0.06250
[epoch: 1, batch:     40] loss: 0.16517 time model: 0.22824 acc: 0.08125
[epoch: 1, batch:     60] loss: 0.16011 time model: 0.22421 acc: 0.09375
[epoch: 1, batch:     80] loss: 0.15752 time model: 0.22220 acc: 0.09531
[epoch: 1, batch:    100] loss: 0.15567 time model: 0.22093 acc: 0.10000
[epoch: 1, batch:    120] loss: 0.15426 time model: 0.22014 acc: 0.10417
[epoch: 1, batch:    140] loss: 0.15207 time model: 0.21956 acc: 0.12098
[epoch: 1, batch:    160] loss: 0.15146 time model: 0.21912 acc: 0.12656
[epoch: 1, batch:    180] loss: 0.15047 time model: 0.21878 acc: 0.13021
[epoch: 1, batch:    200] loss: 0.14985 time model: 0.21852 acc: 0.13406
[epoch: 1, batch:    220] loss: 0.14900 time model: 0.21831 acc: 0.13693
[epoch: 1, batch:    240] loss: 0.14788 time model: 0.21813 acc: 0.14271
[epoch: 1, batch:    260] loss: 0.14732 time model: 0.21798 acc: 0.14351
[epoch: 1, batch:    280] loss: 0.14659 time model: 0.21787 acc: 0.14911
[epoch: 1, batch:    300] loss: 0.14582 time model: 0.21776 acc: 0.15354
[epoch: 1, batch:    320] loss: 0.14491 time model: 0.21767 acc: 0.15762
[epoch: 1, batch:    340] loss: 0.14411 time model: 0.21759 acc: 0.16140
[epoch: 1, batch:    360] loss: 0.14336 time model: 0.21750 acc: 0.16389
[epoch: 1, batch:    380] loss: 0.14233 time model: 0.21743 acc: 0.16974
[epoch: 1, batch:    400] loss: 0.14151 time model: 0.21736 acc: 0.17359
[epoch: 1, batch:    420] loss: 0.14064 time model: 0.21730 acc: 0.17813
[epoch: 1, batch:    440] loss: 0.13994 time model: 0.21724 acc: 0.18281
[epoch: 1, batch:    460] loss: 0.13905 time model: 0.21720 acc: 0.18668
[epoch: 1, batch:    480] loss: 0.13821 time model: 0.21716 acc: 0.19102
[epoch: 1, batch:    500] loss: 0.13738 time model: 0.21711 acc: 0.19300
[epoch: 1, batch:    520] loss: 0.13670 time model: 0.21708 acc: 0.19495
[epoch: 1, batch:    540] loss: 0.13650 time model: 0.21704 acc: 0.19549
[epoch: 1, batch:    560] loss: 0.13601 time model: 0.21700 acc: 0.19866
[epoch: 1, batch:    580] loss: 0.13556 time model: 0.21697 acc: 0.20000
[epoch: 1, batch:    600] loss: 0.13507 time model: 0.21694 acc: 0.20083
[epoch: 1, batch:    620] loss: 0.13448 time model: 0.21691 acc: 0.20363
[epoch: 1, batch:    640] loss: 0.13380 time model: 0.21689 acc: 0.20635
[epoch: 1, batch:    660] loss: 0.13316 time model: 0.21686 acc: 0.20843
[epoch: 1, batch:    680] loss: 0.13259 time model: 0.21684 acc: 0.21112
[epoch: 1, batch:    700] loss: 0.13232 time model: 0.21681 acc: 0.21152
[epoch: 1, batch:    720] loss: 0.13205 time model: 0.21679 acc: 0.21285
[epoch: 1, batch:    740] loss: 0.13164 time model: 0.21676 acc: 0.21427
[epoch: 1, batch:    760] loss: 0.13115 time model: 0.21674 acc: 0.21595
[epoch: 1, batch:    780] loss: 0.13097 time model: 0.21673 acc: 0.21667
[epoch: 1, batch:    800] loss: 0.13059 time model: 0.21671 acc: 0.21891
[epoch: 1, batch:    820] loss: 0.13017 time model: 0.21670 acc: 0.22134
[epoch: 1, batch:    840] loss: 0.12972 time model: 0.21668 acc: 0.22314
[epoch: 1, batch:    860] loss: 0.12934 time model: 0.21667 acc: 0.22413
[epoch: 1, batch:    880] loss: 0.12905 time model: 0.21666 acc: 0.22550
[epoch: 1, batch:    900] loss: 0.12878 time model: 0.21665 acc: 0.22632
[epoch: 1, batch:    920] loss: 0.12848 time model: 0.21663 acc: 0.22717
[epoch: 1, batch:    940] loss: 0.12809 time model: 0.21662 acc: 0.22992
[epoch: 1, batch:    960] loss: 0.12772 time model: 0.21661 acc: 0.23190
[epoch: 1, batch:    980] loss: 0.12743 time model: 0.21661 acc: 0.23304
[epoch: 1, batch:   1000] loss: 0.12716 time model: 0.21659 acc: 0.23556
[epoch: 1, batch:   1020] loss: 0.12683 time model: 0.21658 acc: 0.23664
[epoch: 1, batch:   1040] loss: 0.12640 time model: 0.21657 acc: 0.23900
[epoch: 1, batch:   1060] loss: 0.12615 time model: 0.21657 acc: 0.23992
[epoch: 1, batch:   1080] loss: 0.12583 time model: 0.21655 acc: 0.24149
[epoch: 1, batch:   1100] loss: 0.12548 time model: 0.21655 acc: 0.24307
[epoch: 1, batch:   1120] loss: 0.12515 time model: 0.21654 acc: 0.24431
[epoch: 1, batch:   1140] loss: 0.12494 time model: 0.21653 acc: 0.24594
[epoch: 1, batch:   1160] loss: 0.12461 time model: 0.21653 acc: 0.24774
[epoch: 1, batch:   1180] loss: 0.12426 time model: 0.21652 acc: 0.24984
[epoch: 1, batch:   1200] loss: 0.12391 time model: 0.21651 acc: 0.25182
[epoch: 1, batch:   1220] loss: 0.12363 time model: 0.21650 acc: 0.25364
[epoch: 1, batch:   1240] loss: 0.12325 time model: 0.21650 acc: 0.25474
[epoch: 1, batch:   1260] loss: 0.12290 time model: 0.21649 acc: 0.25615
[epoch: 1, batch:   1280] loss: 0.12254 time model: 0.21648 acc: 0.25781
[epoch: 1, batch:   1300] loss: 0.12217 time model: 0.21647 acc: 0.26019
[epoch: 1, batch:   1320] loss: 0.12188 time model: 0.21647 acc: 0.26269
[epoch: 1, batch:     20] loss: 0.14638 time model: 0.21581 acc: 0.33125
[epoch: 1, batch:     40] loss: 0.14323 time model: 0.21287 acc: 0.35625
[epoch: 1, batch:     60] loss: 0.14389 time model: 0.21168 acc: 0.35625
[epoch: 1, batch:     80] loss: 0.14594 time model: 0.21112 acc: 0.34297
[epoch: 1, batch:    100] loss: 0.14586 time model: 0.21075 acc: 0.33687
[epoch: 1, batch:    120] loss: 0.14465 time model: 0.21049 acc: 0.34323
[epoch: 1, batch:    140] loss: 0.14463 time model: 0.21028 acc: 0.34420
[epoch: 1, batch:    160] loss: 0.14537 time model: 0.21014 acc: 0.34336
epoch:1 train loss: 0.12188291537820996 train acc: 0.2626865671641791 valid loss: 0.14654793034332594 valid acc: 0.3426464881645324
[epoch: 2, batch:     20] loss: 0.11237 time model: 0.23080 acc: 0.31875
[epoch: 2, batch:     40] loss: 0.10957 time model: 0.22377 acc: 0.33437
[epoch: 2, batch:     60] loss: 0.10620 time model: 0.22124 acc: 0.34063
[epoch: 2, batch:     80] loss: 0.10458 time model: 0.22003 acc: 0.35391
[epoch: 2, batch:    100] loss: 0.10447 time model: 0.21926 acc: 0.35500
[epoch: 2, batch:    120] loss: 0.10357 time model: 0.21875 acc: 0.36198
[epoch: 2, batch:    140] loss: 0.10279 time model: 0.21836 acc: 0.36205
[epoch: 2, batch:    160] loss: 0.10260 time model: 0.21807 acc: 0.36445
[epoch: 2, batch:    180] loss: 0.10176 time model: 0.21786 acc: 0.36701
[epoch: 2, batch:    200] loss: 0.10128 time model: 0.21770 acc: 0.37094
[epoch: 2, batch:    220] loss: 0.10187 time model: 0.21755 acc: 0.36875
[epoch: 2, batch:    240] loss: 0.10226 time model: 0.21744 acc: 0.36823
[epoch: 2, batch:    260] loss: 0.10183 time model: 0.21734 acc: 0.37139
[epoch: 2, batch:    280] loss: 0.10134 time model: 0.21725 acc: 0.37723
[epoch: 2, batch:    300] loss: 0.10098 time model: 0.21717 acc: 0.38062
[epoch: 2, batch:    320] loss: 0.09996 time model: 0.21711 acc: 0.38594
[epoch: 2, batch:    340] loss: 0.09963 time model: 0.21705 acc: 0.38787
[epoch: 2, batch:    360] loss: 0.09967 time model: 0.21700 acc: 0.38854
[epoch: 2, batch:    380] loss: 0.09954 time model: 0.21695 acc: 0.38914
[epoch: 2, batch:    400] loss: 0.09950 time model: 0.21690 acc: 0.38938
[epoch: 2, batch:    420] loss: 0.09942 time model: 0.21686 acc: 0.39018
[epoch: 2, batch:    440] loss: 0.09921 time model: 0.21683 acc: 0.39304
[epoch: 2, batch:    460] loss: 0.09888 time model: 0.21679 acc: 0.39402
[epoch: 2, batch:    480] loss: 0.09846 time model: 0.21676 acc: 0.39687
[epoch: 2, batch:    500] loss: 0.09804 time model: 0.21674 acc: 0.39987
[epoch: 2, batch:    520] loss: 0.09742 time model: 0.21670 acc: 0.40240
[epoch: 2, batch:    540] loss: 0.09712 time model: 0.21667 acc: 0.40370
[epoch: 2, batch:    560] loss: 0.09694 time model: 0.21665 acc: 0.40480
[epoch: 2, batch:    580] loss: 0.09677 time model: 0.21662 acc: 0.40474
[epoch: 2, batch:    600] loss: 0.09658 time model: 0.21660 acc: 0.40563
[epoch: 2, batch:    620] loss: 0.09620 time model: 0.21657 acc: 0.40837
[epoch: 2, batch:    640] loss: 0.09593 time model: 0.21655 acc: 0.40957
[epoch: 2, batch:    660] loss: 0.09560 time model: 0.21654 acc: 0.41193
[epoch: 2, batch:    680] loss: 0.09551 time model: 0.21653 acc: 0.41186
[epoch: 2, batch:    700] loss: 0.09536 time model: 0.21649 acc: 0.41268
[epoch: 2, batch:    720] loss: 0.09486 time model: 0.21645 acc: 0.41536
[epoch: 2, batch:    740] loss: 0.09476 time model: 0.21641 acc: 0.41579
[epoch: 2, batch:    760] loss: 0.09450 time model: 0.21632 acc: 0.41711
[epoch: 2, batch:    780] loss: 0.09429 time model: 0.21625 acc: 0.41795
[epoch: 2, batch:    800] loss: 0.09418 time model: 0.21621 acc: 0.41883
[epoch: 2, batch:    820] loss: 0.09402 time model: 0.21619 acc: 0.41905
[epoch: 2, batch:    840] loss: 0.09375 time model: 0.21617 acc: 0.42076
[epoch: 2, batch:    860] loss: 0.09352 time model: 0.21615 acc: 0.42267
[epoch: 2, batch:    880] loss: 0.09329 time model: 0.21612 acc: 0.42450
[epoch: 2, batch:    900] loss: 0.09305 time model: 0.21611 acc: 0.42556
[epoch: 2, batch:    920] loss: 0.09296 time model: 0.21609 acc: 0.42622
[epoch: 2, batch:    940] loss: 0.09281 time model: 0.21606 acc: 0.42699
[epoch: 2, batch:    960] loss: 0.09257 time model: 0.21604 acc: 0.42812
[epoch: 2, batch:    980] loss: 0.09243 time model: 0.21600 acc: 0.42825
[epoch: 2, batch:   1000] loss: 0.09219 time model: 0.21598 acc: 0.42950
[epoch: 2, batch:   1020] loss: 0.09190 time model: 0.21597 acc: 0.43094
[epoch: 2, batch:   1040] loss: 0.09176 time model: 0.21596 acc: 0.43131
[epoch: 2, batch:   1060] loss: 0.09142 time model: 0.21596 acc: 0.43367
[epoch: 2, batch:   1080] loss: 0.09124 time model: 0.21596 acc: 0.43449
[epoch: 2, batch:   1100] loss: 0.09112 time model: 0.21595 acc: 0.43562
[epoch: 2, batch:   1120] loss: 0.09095 time model: 0.21595 acc: 0.43638
[epoch: 2, batch:   1140] loss: 0.09077 time model: 0.21594 acc: 0.43695
[epoch: 2, batch:   1160] loss: 0.09074 time model: 0.21594 acc: 0.43691
[epoch: 2, batch:   1180] loss: 0.09048 time model: 0.21594 acc: 0.43787
[epoch: 2, batch:   1200] loss: 0.09037 time model: 0.21593 acc: 0.43880
[epoch: 2, batch:   1220] loss: 0.09011 time model: 0.21593 acc: 0.44057
[epoch: 2, batch:   1240] loss: 0.08990 time model: 0.21593 acc: 0.44153
[epoch: 2, batch:   1260] loss: 0.08976 time model: 0.21592 acc: 0.44251
[epoch: 2, batch:   1280] loss: 0.08957 time model: 0.21592 acc: 0.44375
[epoch: 2, batch:   1300] loss: 0.08943 time model: 0.21591 acc: 0.44514
[epoch: 2, batch:   1320] loss: 0.08939 time model: 0.21591 acc: 0.44558
[epoch: 2, batch:     20] loss: 0.07845 time model: 0.21555 acc: 0.51875
[epoch: 2, batch:     40] loss: 0.07913 time model: 0.21261 acc: 0.51875
[epoch: 2, batch:     60] loss: 0.07899 time model: 0.21141 acc: 0.53333
[epoch: 2, batch:     80] loss: 0.07743 time model: 0.21065 acc: 0.54375
[epoch: 2, batch:    100] loss: 0.07869 time model: 0.21006 acc: 0.53312
[epoch: 2, batch:    120] loss: 0.07880 time model: 0.20966 acc: 0.53333
[epoch: 2, batch:    140] loss: 0.07774 time model: 0.20919 acc: 0.53973
[epoch: 2, batch:    160] loss: 0.07770 time model: 0.20873 acc: 0.53789
epoch:2 train loss: 0.08938621453723758 train acc: 0.44558161573086946 valid loss: 0.07792415485208147 valid acc: 0.5386107877376795
[epoch: 3, batch:     20] loss: 0.07187 time model: 0.22903 acc: 0.56875
[epoch: 3, batch:     40] loss: 0.07339 time model: 0.22090 acc: 0.55156
[epoch: 3, batch:     60] loss: 0.07248 time model: 0.21798 acc: 0.56354
[epoch: 3, batch:     80] loss: 0.07418 time model: 0.21657 acc: 0.54766
[epoch: 3, batch:    100] loss: 0.07440 time model: 0.21580 acc: 0.54125
[epoch: 3, batch:    120] loss: 0.07408 time model: 0.21530 acc: 0.54427
[epoch: 3, batch:    140] loss: 0.07422 time model: 0.21494 acc: 0.54241
[epoch: 3, batch:    160] loss: 0.07471 time model: 0.21467 acc: 0.53906
[epoch: 3, batch:    180] loss: 0.07485 time model: 0.21472 acc: 0.53611
[epoch: 3, batch:    200] loss: 0.07435 time model: 0.21480 acc: 0.53656
[epoch: 3, batch:    220] loss: 0.07461 time model: 0.21488 acc: 0.53722
[epoch: 3, batch:    240] loss: 0.07468 time model: 0.21496 acc: 0.53750
[epoch: 3, batch:    260] loss: 0.07505 time model: 0.21504 acc: 0.53558
[epoch: 3, batch:    280] loss: 0.07509 time model: 0.21510 acc: 0.53549
[epoch: 3, batch:    300] loss: 0.07558 time model: 0.21513 acc: 0.53187
[epoch: 3, batch:    320] loss: 0.07547 time model: 0.21517 acc: 0.53242
[epoch: 3, batch:    340] loss: 0.07531 time model: 0.21521 acc: 0.53364
[epoch: 3, batch:    360] loss: 0.07535 time model: 0.21523 acc: 0.53490
[epoch: 3, batch:    380] loss: 0.07509 time model: 0.21526 acc: 0.53602
[epoch: 3, batch:    400] loss: 0.07507 time model: 0.21529 acc: 0.53563
[epoch: 3, batch:    420] loss: 0.07471 time model: 0.21533 acc: 0.53854
[epoch: 3, batch:    440] loss: 0.07471 time model: 0.21535 acc: 0.53793
[epoch: 3, batch:    460] loss: 0.07485 time model: 0.21537 acc: 0.53736
[epoch: 3, batch:    480] loss: 0.07467 time model: 0.21540 acc: 0.53815
[epoch: 3, batch:    500] loss: 0.07454 time model: 0.21542 acc: 0.53912
[epoch: 3, batch:    520] loss: 0.07427 time model: 0.21545 acc: 0.54147
[epoch: 3, batch:    540] loss: 0.07425 time model: 0.21547 acc: 0.54178
[epoch: 3, batch:    560] loss: 0.07429 time model: 0.21549 acc: 0.54152
[epoch: 3, batch:    580] loss: 0.07421 time model: 0.21550 acc: 0.54149
[epoch: 3, batch:    600] loss: 0.07417 time model: 0.21551 acc: 0.54219
[epoch: 3, batch:    620] loss: 0.07389 time model: 0.21553 acc: 0.54315
[epoch: 3, batch:    640] loss: 0.07369 time model: 0.21554 acc: 0.54570
[epoch: 3, batch:    660] loss: 0.07341 time model: 0.21556 acc: 0.54782
[epoch: 3, batch:    680] loss: 0.07358 time model: 0.21557 acc: 0.54816
[epoch: 3, batch:    700] loss: 0.07352 time model: 0.21558 acc: 0.54848
[epoch: 3, batch:    720] loss: 0.07344 time model: 0.21560 acc: 0.54870
[epoch: 3, batch:    740] loss: 0.07351 time model: 0.21560 acc: 0.54806
[epoch: 3, batch:    760] loss: 0.07355 time model: 0.21561 acc: 0.54885
[epoch: 3, batch:    780] loss: 0.07358 time model: 0.21562 acc: 0.54936
[epoch: 3, batch:    800] loss: 0.07366 time model: 0.21563 acc: 0.54883
[epoch: 3, batch:    820] loss: 0.07360 time model: 0.21564 acc: 0.54886
[epoch: 3, batch:    840] loss: 0.07364 time model: 0.21565 acc: 0.54881
[epoch: 3, batch:    860] loss: 0.07361 time model: 0.21565 acc: 0.55015
[epoch: 3, batch:    880] loss: 0.07364 time model: 0.21566 acc: 0.55014
[epoch: 3, batch:    900] loss: 0.07365 time model: 0.21566 acc: 0.55035
[epoch: 3, batch:    920] loss: 0.07361 time model: 0.21567 acc: 0.55082
[epoch: 3, batch:    940] loss: 0.07358 time model: 0.21567 acc: 0.55140
[epoch: 3, batch:    960] loss: 0.07354 time model: 0.21566 acc: 0.55202
[epoch: 3, batch:    980] loss: 0.07345 time model: 0.21564 acc: 0.55230
[epoch: 3, batch:   1000] loss: 0.07340 time model: 0.21564 acc: 0.55306
[epoch: 3, batch:   1020] loss: 0.07323 time model: 0.21564 acc: 0.55417
[epoch: 3, batch:   1040] loss: 0.07307 time model: 0.21564 acc: 0.55559
[epoch: 3, batch:   1060] loss: 0.07301 time model: 0.21564 acc: 0.55566
[epoch: 3, batch:   1080] loss: 0.07297 time model: 0.21562 acc: 0.55590
[epoch: 3, batch:   1100] loss: 0.07293 time model: 0.21562 acc: 0.55631
[epoch: 3, batch:   1120] loss: 0.07295 time model: 0.21561 acc: 0.55608
[epoch: 3, batch:   1140] loss: 0.07279 time model: 0.21559 acc: 0.55685
[epoch: 3, batch:   1160] loss: 0.07275 time model: 0.21556 acc: 0.55727
[epoch: 3, batch:   1180] loss: 0.07265 time model: 0.21555 acc: 0.55800
[epoch: 3, batch:   1200] loss: 0.07277 time model: 0.21551 acc: 0.55740
[epoch: 3, batch:   1220] loss: 0.07263 time model: 0.21546 acc: 0.55820
[epoch: 3, batch:   1240] loss: 0.07254 time model: 0.21541 acc: 0.55791
[epoch: 3, batch:   1260] loss: 0.07249 time model: 0.21536 acc: 0.55848
[epoch: 3, batch:   1280] loss: 0.07247 time model: 0.21532 acc: 0.55889
[epoch: 3, batch:   1300] loss: 0.07244 time model: 0.21530 acc: 0.55885
[epoch: 3, batch:   1320] loss: 0.07247 time model: 0.21526 acc: 0.55944
[epoch: 3, batch:     20] loss: 0.07730 time model: 0.21084 acc: 0.55625
[epoch: 3, batch:     40] loss: 0.08137 time model: 0.20839 acc: 0.54219
[epoch: 3, batch:     60] loss: 0.08137 time model: 0.20763 acc: 0.53542
[epoch: 3, batch:     80] loss: 0.07941 time model: 0.20772 acc: 0.54297
[epoch: 3, batch:    100] loss: 0.07962 time model: 0.20780 acc: 0.53875
[epoch: 3, batch:    120] loss: 0.08038 time model: 0.20786 acc: 0.53698
[epoch: 3, batch:    140] loss: 0.07970 time model: 0.20787 acc: 0.53884
[epoch: 3, batch:    160] loss: 0.08027 time model: 0.20788 acc: 0.53398
epoch:3 train loss: 0.07246799936539695 train acc: 0.5594408907841744 valid loss: 0.08162712745533278 valid acc: 0.5331781140861467
[epoch: 4, batch:     20] loss: 0.05936 time model: 0.22889 acc: 0.65312
[epoch: 4, batch:     40] loss: 0.06260 time model: 0.22262 acc: 0.63125
[epoch: 4, batch:     60] loss: 0.06315 time model: 0.22021 acc: 0.61250
[epoch: 4, batch:     80] loss: 0.06469 time model: 0.21908 acc: 0.61641
[epoch: 4, batch:    100] loss: 0.06461 time model: 0.21837 acc: 0.61125
[epoch: 4, batch:    120] loss: 0.06342 time model: 0.21785 acc: 0.61927
[epoch: 4, batch:    140] loss: 0.06339 time model: 0.21754 acc: 0.62054
[epoch: 4, batch:    160] loss: 0.06329 time model: 0.21732 acc: 0.61328
[epoch: 4, batch:    180] loss: 0.06406 time model: 0.21710 acc: 0.61111
[epoch: 4, batch:    200] loss: 0.06362 time model: 0.21693 acc: 0.61125
[epoch: 4, batch:    220] loss: 0.06369 time model: 0.21678 acc: 0.61136
[epoch: 4, batch:    240] loss: 0.06390 time model: 0.21663 acc: 0.60911
[epoch: 4, batch:    260] loss: 0.06393 time model: 0.21652 acc: 0.61106
[epoch: 4, batch:    280] loss: 0.06435 time model: 0.21643 acc: 0.60893
[epoch: 4, batch:    300] loss: 0.06431 time model: 0.21636 acc: 0.61000
[epoch: 4, batch:    320] loss: 0.06415 time model: 0.21630 acc: 0.61172
[epoch: 4, batch:    340] loss: 0.06425 time model: 0.21624 acc: 0.61232
[epoch: 4, batch:    360] loss: 0.06441 time model: 0.21618 acc: 0.61233
[epoch: 4, batch:    380] loss: 0.06455 time model: 0.21614 acc: 0.61184
[epoch: 4, batch:    400] loss: 0.06461 time model: 0.21609 acc: 0.61062
[epoch: 4, batch:    420] loss: 0.06439 time model: 0.21606 acc: 0.61176
[epoch: 4, batch:    440] loss: 0.06453 time model: 0.21602 acc: 0.61051
[epoch: 4, batch:    460] loss: 0.06447 time model: 0.21598 acc: 0.60938
[epoch: 4, batch:    480] loss: 0.06416 time model: 0.21595 acc: 0.61172
[epoch: 4, batch:    500] loss: 0.06401 time model: 0.21593 acc: 0.61450
[epoch: 4, batch:    520] loss: 0.06392 time model: 0.21590 acc: 0.61550
[epoch: 4, batch:    540] loss: 0.06374 time model: 0.21587 acc: 0.61516
[epoch: 4, batch:    560] loss: 0.06374 time model: 0.21585 acc: 0.61462
[epoch: 4, batch:    580] loss: 0.06371 time model: 0.21581 acc: 0.61466
[epoch: 4, batch:    600] loss: 0.06377 time model: 0.21576 acc: 0.61333
[epoch: 4, batch:    620] loss: 0.06396 time model: 0.21571 acc: 0.61169
[epoch: 4, batch:    640] loss: 0.06379 time model: 0.21562 acc: 0.61387
[epoch: 4, batch:    660] loss: 0.06365 time model: 0.21552 acc: 0.61506
[epoch: 4, batch:    680] loss: 0.06357 time model: 0.21543 acc: 0.61627
[epoch: 4, batch:    700] loss: 0.06362 time model: 0.21534 acc: 0.61554
[epoch: 4, batch:    720] loss: 0.06383 time model: 0.21525 acc: 0.61510
[epoch: 4, batch:    740] loss: 0.06376 time model: 0.21518 acc: 0.61546
[epoch: 4, batch:    760] loss: 0.06363 time model: 0.21510 acc: 0.61678
[epoch: 4, batch:    780] loss: 0.06373 time model: 0.21504 acc: 0.61611
[epoch: 4, batch:    800] loss: 0.06381 time model: 0.21498 acc: 0.61539
[epoch: 4, batch:    820] loss: 0.06381 time model: 0.21493 acc: 0.61578
[epoch: 4, batch:    840] loss: 0.06384 time model: 0.21488 acc: 0.61600
[epoch: 4, batch:    860] loss: 0.06384 time model: 0.21484 acc: 0.61599
[epoch: 4, batch:    880] loss: 0.06376 time model: 0.21480 acc: 0.61705
[epoch: 4, batch:    900] loss: 0.06366 time model: 0.21476 acc: 0.61764
[epoch: 4, batch:    920] loss: 0.06367 time model: 0.21473 acc: 0.61793
[epoch: 4, batch:    940] loss: 0.06370 time model: 0.21469 acc: 0.61769
[epoch: 4, batch:    960] loss: 0.06372 time model: 0.21466 acc: 0.61803
[epoch: 4, batch:    980] loss: 0.06372 time model: 0.21463 acc: 0.61869
[epoch: 4, batch:   1000] loss: 0.06375 time model: 0.21459 acc: 0.61969
[epoch: 4, batch:   1020] loss: 0.06376 time model: 0.21456 acc: 0.61961
[epoch: 4, batch:   1040] loss: 0.06376 time model: 0.21457 acc: 0.62043
[epoch: 4, batch:   1060] loss: 0.06376 time model: 0.21459 acc: 0.62046
[epoch: 4, batch:   1080] loss: 0.06383 time model: 0.21461 acc: 0.62037
[epoch: 4, batch:   1100] loss: 0.06370 time model: 0.21463 acc: 0.62074
[epoch: 4, batch:   1120] loss: 0.06351 time model: 0.21465 acc: 0.62171
[epoch: 4, batch:   1140] loss: 0.06335 time model: 0.21466 acc: 0.62286
[epoch: 4, batch:   1160] loss: 0.06334 time model: 0.21468 acc: 0.62258
[epoch: 4, batch:   1180] loss: 0.06335 time model: 0.21468 acc: 0.62214
[epoch: 4, batch:   1200] loss: 0.06334 time model: 0.21467 acc: 0.62182
[epoch: 4, batch:   1220] loss: 0.06331 time model: 0.21467 acc: 0.62198
[epoch: 4, batch:   1240] loss: 0.06328 time model: 0.21467 acc: 0.62258
[epoch: 4, batch:   1260] loss: 0.06322 time model: 0.21467 acc: 0.62316
[epoch: 4, batch:   1280] loss: 0.06314 time model: 0.21467 acc: 0.62358
[epoch: 4, batch:   1300] loss: 0.06315 time model: 0.21468 acc: 0.62375
[epoch: 4, batch:   1320] loss: 0.06328 time model: 0.21470 acc: 0.62355
[epoch: 4, batch:     20] loss: 0.08716 time model: 0.21318 acc: 0.60313
[epoch: 4, batch:     40] loss: 0.09042 time model: 0.21105 acc: 0.57344
[epoch: 4, batch:     60] loss: 0.08991 time model: 0.21027 acc: 0.55833
[epoch: 4, batch:     80] loss: 0.09023 time model: 0.20985 acc: 0.55469
[epoch: 4, batch:    100] loss: 0.08825 time model: 0.20959 acc: 0.55937
[epoch: 4, batch:    120] loss: 0.08739 time model: 0.20936 acc: 0.56146
[epoch: 4, batch:    140] loss: 0.08828 time model: 0.20923 acc: 0.56116
[epoch: 4, batch:    160] loss: 0.08692 time model: 0.20913 acc: 0.57031
epoch:4 train loss: 0.06327978732235141 train acc: 0.6235489220563848 valid loss: 0.08801262461727426 valid acc: 0.5712068296468762
[epoch: 5, batch:     20] loss: 0.06677 time model: 0.23255 acc: 0.62187
[epoch: 5, batch:     40] loss: 0.06564 time model: 0.22393 acc: 0.61875
[epoch: 5, batch:     60] loss: 0.06697 time model: 0.22109 acc: 0.61875
[epoch: 5, batch:     80] loss: 0.06624 time model: 0.21967 acc: 0.61406
[epoch: 5, batch:    100] loss: 0.06531 time model: 0.21881 acc: 0.61938
[epoch: 5, batch:    120] loss: 0.06419 time model: 0.21823 acc: 0.62708
[epoch: 5, batch:    140] loss: 0.06304 time model: 0.21781 acc: 0.62946
[epoch: 5, batch:    160] loss: 0.06252 time model: 0.21752 acc: 0.63008
[epoch: 5, batch:    180] loss: 0.06193 time model: 0.21729 acc: 0.63194
[epoch: 5, batch:    200] loss: 0.06249 time model: 0.21711 acc: 0.62625
[epoch: 5, batch:    220] loss: 0.06228 time model: 0.21695 acc: 0.62670
[epoch: 5, batch:    240] loss: 0.06156 time model: 0.21685 acc: 0.63073
[epoch: 5, batch:    260] loss: 0.06146 time model: 0.21676 acc: 0.63125
[epoch: 5, batch:    280] loss: 0.06151 time model: 0.21668 acc: 0.63348
[epoch: 5, batch:    300] loss: 0.06126 time model: 0.21662 acc: 0.63646
[epoch: 5, batch:    320] loss: 0.06061 time model: 0.21655 acc: 0.63789
[epoch: 5, batch:    340] loss: 0.06045 time model: 0.21649 acc: 0.63640
[epoch: 5, batch:    360] loss: 0.06046 time model: 0.21645 acc: 0.63559
[epoch: 5, batch:    380] loss: 0.06001 time model: 0.21640 acc: 0.63947
[epoch: 5, batch:    400] loss: 0.05954 time model: 0.21637 acc: 0.64250
[epoch: 5, batch:    420] loss: 0.05951 time model: 0.21633 acc: 0.64420
[epoch: 5, batch:    440] loss: 0.05947 time model: 0.21630 acc: 0.64361
[epoch: 5, batch:    460] loss: 0.05916 time model: 0.21628 acc: 0.64552
[epoch: 5, batch:    480] loss: 0.05924 time model: 0.21625 acc: 0.64505
[epoch: 5, batch:    500] loss: 0.05889 time model: 0.21623 acc: 0.64663
[epoch: 5, batch:    520] loss: 0.05874 time model: 0.21621 acc: 0.64832
[epoch: 5, batch:    540] loss: 0.05872 time model: 0.21620 acc: 0.64919
[epoch: 5, batch:    560] loss: 0.05883 time model: 0.21618 acc: 0.64844
[epoch: 5, batch:    580] loss: 0.05883 time model: 0.21616 acc: 0.64881
[epoch: 5, batch:    600] loss: 0.05873 time model: 0.21615 acc: 0.64875
[epoch: 5, batch:    620] loss: 0.05867 time model: 0.21614 acc: 0.64829
[epoch: 5, batch:    640] loss: 0.05870 time model: 0.21612 acc: 0.64873
[epoch: 5, batch:    660] loss: 0.05876 time model: 0.21611 acc: 0.64943
[epoch: 5, batch:    680] loss: 0.05878 time model: 0.21610 acc: 0.65028
[epoch: 5, batch:    700] loss: 0.05884 time model: 0.21609 acc: 0.64973
[epoch: 5, batch:    720] loss: 0.05873 time model: 0.21607 acc: 0.64852
[epoch: 5, batch:    740] loss: 0.05866 time model: 0.21606 acc: 0.64890
[epoch: 5, batch:    760] loss: 0.05852 time model: 0.21606 acc: 0.64918
[epoch: 5, batch:    780] loss: 0.05860 time model: 0.21605 acc: 0.64952
[epoch: 5, batch:    800] loss: 0.05847 time model: 0.21604 acc: 0.65016
[epoch: 5, batch:    820] loss: 0.05858 time model: 0.21603 acc: 0.64916
[epoch: 5, batch:    840] loss: 0.05841 time model: 0.21602 acc: 0.65037
[epoch: 5, batch:    860] loss: 0.05816 time model: 0.21601 acc: 0.65153
[epoch: 5, batch:    880] loss: 0.05806 time model: 0.21600 acc: 0.65199
[epoch: 5, batch:    900] loss: 0.05802 time model: 0.21600 acc: 0.65174
[epoch: 5, batch:    920] loss: 0.05815 time model: 0.21599 acc: 0.65102
[epoch: 5, batch:    940] loss: 0.05809 time model: 0.21599 acc: 0.65126
[epoch: 5, batch:    960] loss: 0.05802 time model: 0.21598 acc: 0.65189
[epoch: 5, batch:    980] loss: 0.05806 time model: 0.21597 acc: 0.65236
[epoch: 5, batch:   1000] loss: 0.05820 time model: 0.21596 acc: 0.65212
[epoch: 5, batch:   1020] loss: 0.05818 time model: 0.21596 acc: 0.65202
[epoch: 5, batch:   1040] loss: 0.05806 time model: 0.21595 acc: 0.65246
[epoch: 5, batch:   1060] loss: 0.05794 time model: 0.21593 acc: 0.65312
[epoch: 5, batch:   1080] loss: 0.05789 time model: 0.21590 acc: 0.65365
[epoch: 5, batch:   1100] loss: 0.05791 time model: 0.21588 acc: 0.65364
[epoch: 5, batch:   1120] loss: 0.05791 time model: 0.21586 acc: 0.65424
[epoch: 5, batch:   1140] loss: 0.05793 time model: 0.21584 acc: 0.65389
[epoch: 5, batch:   1160] loss: 0.05787 time model: 0.21581 acc: 0.65420
[epoch: 5, batch:   1180] loss: 0.05785 time model: 0.21579 acc: 0.65445
[epoch: 5, batch:   1200] loss: 0.05790 time model: 0.21577 acc: 0.65411
[epoch: 5, batch:   1220] loss: 0.05772 time model: 0.21575 acc: 0.65538
[epoch: 5, batch:   1240] loss: 0.05765 time model: 0.21572 acc: 0.65585
[epoch: 5, batch:   1260] loss: 0.05767 time model: 0.21570 acc: 0.65565
[epoch: 5, batch:   1280] loss: 0.05768 time model: 0.21568 acc: 0.65576
[epoch: 5, batch:   1300] loss: 0.05758 time model: 0.21566 acc: 0.65620
[epoch: 5, batch:   1320] loss: 0.05783 time model: 0.21565 acc: 0.65615
[epoch: 5, batch:     20] loss: 0.10193 time model: 0.21190 acc: 0.55937
[epoch: 5, batch:     40] loss: 0.09152 time model: 0.21011 acc: 0.58125
[epoch: 5, batch:     60] loss: 0.08931 time model: 0.20937 acc: 0.59896
[epoch: 5, batch:     80] loss: 0.09072 time model: 0.20924 acc: 0.59375
[epoch: 5, batch:    100] loss: 0.09261 time model: 0.20924 acc: 0.58937
[epoch: 5, batch:    120] loss: 0.09459 time model: 0.20924 acc: 0.58177
[epoch: 5, batch:    140] loss: 0.09298 time model: 0.20919 acc: 0.58884
[epoch: 5, batch:    160] loss: 0.09299 time model: 0.20914 acc: 0.58711
epoch:5 train loss: 0.0578303716174801 train acc: 0.6561478322672353 valid loss: 0.09287940101610583 valid acc: 0.5875048506014746
[epoch: 6, batch:     20] loss: 0.05869 time model: 0.23074 acc: 0.68125
[epoch: 6, batch:     40] loss: 0.05954 time model: 0.22356 acc: 0.65938
[epoch: 6, batch:     60] loss: 0.05845 time model: 0.22096 acc: 0.66042
[epoch: 6, batch:     80] loss: 0.05730 time model: 0.21946 acc: 0.66172
[epoch: 6, batch:    100] loss: 0.05832 time model: 0.21856 acc: 0.65875
[epoch: 6, batch:    120] loss: 0.05841 time model: 0.21782 acc: 0.65417
[epoch: 6, batch:    140] loss: 0.05771 time model: 0.21711 acc: 0.65938
[epoch: 6, batch:    160] loss: 0.05726 time model: 0.21656 acc: 0.66055
[epoch: 6, batch:    180] loss: 0.05683 time model: 0.21611 acc: 0.66250
[epoch: 6, batch:    200] loss: 0.05622 time model: 0.21575 acc: 0.66687
[epoch: 6, batch:    220] loss: 0.05536 time model: 0.21549 acc: 0.67216
[epoch: 6, batch:    240] loss: 0.05511 time model: 0.21524 acc: 0.67266
[epoch: 6, batch:    260] loss: 0.05532 time model: 0.21503 acc: 0.67091
[epoch: 6, batch:    280] loss: 0.05519 time model: 0.21483 acc: 0.67143
[epoch: 6, batch:    300] loss: 0.05519 time model: 0.21468 acc: 0.67000
[epoch: 6, batch:    320] loss: 0.05512 time model: 0.21454 acc: 0.67070
[epoch: 6, batch:    340] loss: 0.05553 time model: 0.21441 acc: 0.66949
[epoch: 6, batch:    360] loss: 0.05521 time model: 0.21431 acc: 0.67083
[epoch: 6, batch:    380] loss: 0.05520 time model: 0.21421 acc: 0.67220
[epoch: 6, batch:    400] loss: 0.05499 time model: 0.21414 acc: 0.67328
[epoch: 6, batch:    420] loss: 0.05511 time model: 0.21407 acc: 0.67083
[epoch: 6, batch:    440] loss: 0.05488 time model: 0.21401 acc: 0.67102
[epoch: 6, batch:    460] loss: 0.05482 time model: 0.21395 acc: 0.67174
[epoch: 6, batch:    480] loss: 0.05468 time model: 0.21400 acc: 0.67331
[epoch: 6, batch:    500] loss: 0.05454 time model: 0.21404 acc: 0.67337
[epoch: 6, batch:    520] loss: 0.05463 time model: 0.21410 acc: 0.67236
[epoch: 6, batch:    540] loss: 0.05437 time model: 0.21416 acc: 0.67442
[epoch: 6, batch:    560] loss: 0.05411 time model: 0.21422 acc: 0.67623
[epoch: 6, batch:    580] loss: 0.05423 time model: 0.21427 acc: 0.67532
[epoch: 6, batch:    600] loss: 0.05415 time model: 0.21430 acc: 0.67490
[epoch: 6, batch:    620] loss: 0.05409 time model: 0.21433 acc: 0.67621
[epoch: 6, batch:    640] loss: 0.05401 time model: 0.21437 acc: 0.67725
[epoch: 6, batch:    660] loss: 0.05409 time model: 0.21439 acc: 0.67633
[epoch: 6, batch:    680] loss: 0.05402 time model: 0.21441 acc: 0.67711
[epoch: 6, batch:    700] loss: 0.05406 time model: 0.21444 acc: 0.67795
[epoch: 6, batch:    720] loss: 0.05390 time model: 0.21448 acc: 0.67908
[epoch: 6, batch:    740] loss: 0.05382 time model: 0.21451 acc: 0.67922
[epoch: 6, batch:    760] loss: 0.05403 time model: 0.21454 acc: 0.67780
[epoch: 6, batch:    780] loss: 0.05414 time model: 0.21457 acc: 0.67780
[epoch: 6, batch:    800] loss: 0.05414 time model: 0.21460 acc: 0.67820
[epoch: 6, batch:    820] loss: 0.05423 time model: 0.21463 acc: 0.67752
[epoch: 6, batch:    840] loss: 0.05411 time model: 0.21466 acc: 0.67872
[epoch: 6, batch:    860] loss: 0.05410 time model: 0.21468 acc: 0.67863
[epoch: 6, batch:    880] loss: 0.05407 time model: 0.21471 acc: 0.67862
[epoch: 6, batch:    900] loss: 0.05410 time model: 0.21474 acc: 0.67868
[epoch: 6, batch:    920] loss: 0.05414 time model: 0.21476 acc: 0.67846
[epoch: 6, batch:    940] loss: 0.05418 time model: 0.21478 acc: 0.67766
[epoch: 6, batch:    960] loss: 0.05409 time model: 0.21480 acc: 0.67812
[epoch: 6, batch:    980] loss: 0.05397 time model: 0.21482 acc: 0.67883
[epoch: 6, batch:   1000] loss: 0.05409 time model: 0.21484 acc: 0.67844
[epoch: 6, batch:   1020] loss: 0.05387 time model: 0.21485 acc: 0.67923
[epoch: 6, batch:   1040] loss: 0.05379 time model: 0.21487 acc: 0.67951
[epoch: 6, batch:   1060] loss: 0.05367 time model: 0.21488 acc: 0.68019
[epoch: 6, batch:   1080] loss: 0.05379 time model: 0.21490 acc: 0.67963
[epoch: 6, batch:   1100] loss: 0.05383 time model: 0.21491 acc: 0.67977
[epoch: 6, batch:   1120] loss: 0.05392 time model: 0.21492 acc: 0.67935
[epoch: 6, batch:   1140] loss: 0.05387 time model: 0.21492 acc: 0.67988
[epoch: 6, batch:   1160] loss: 0.05396 time model: 0.21493 acc: 0.67877
[epoch: 6, batch:   1180] loss: 0.05393 time model: 0.21493 acc: 0.67850
[epoch: 6, batch:   1200] loss: 0.05387 time model: 0.21494 acc: 0.67870
[epoch: 6, batch:   1220] loss: 0.05386 time model: 0.21495 acc: 0.67864
[epoch: 6, batch:   1240] loss: 0.05388 time model: 0.21496 acc: 0.67918
[epoch: 6, batch:   1260] loss: 0.05390 time model: 0.21497 acc: 0.67922
[epoch: 6, batch:   1280] loss: 0.05392 time model: 0.21498 acc: 0.67920
[epoch: 6, batch:   1300] loss: 0.05387 time model: 0.21499 acc: 0.68005
[epoch: 6, batch:   1320] loss: 0.05380 time model: 0.21501 acc: 0.68083
[epoch: 6, batch:     20] loss: 0.08779 time model: 0.21529 acc: 0.59062
[epoch: 6, batch:     40] loss: 0.09264 time model: 0.21243 acc: 0.56719
[epoch: 6, batch:     60] loss: 0.09228 time model: 0.21131 acc: 0.55729
[epoch: 6, batch:     80] loss: 0.09210 time model: 0.21075 acc: 0.56328
[epoch: 6, batch:    100] loss: 0.09204 time model: 0.21035 acc: 0.57000
[epoch: 6, batch:    120] loss: 0.09348 time model: 0.21012 acc: 0.55990
[epoch: 6, batch:    140] loss: 0.09397 time model: 0.20992 acc: 0.56295
[epoch: 6, batch:    160] loss: 0.09426 time model: 0.20978 acc: 0.55977
epoch:6 train loss: 0.05380329270267961 train acc: 0.680833925610045 valid loss: 0.09525231740064846 valid acc: 0.559953434225844
[epoch: 7, batch:     20] loss: 0.04784 time model: 0.22761 acc: 0.68437
[epoch: 7, batch:     40] loss: 0.05189 time model: 0.22188 acc: 0.66250
[epoch: 7, batch:     60] loss: 0.05180 time model: 0.21987 acc: 0.67812
[epoch: 7, batch:     80] loss: 0.05136 time model: 0.21884 acc: 0.67734
[epoch: 7, batch:    100] loss: 0.05105 time model: 0.21823 acc: 0.68188
[epoch: 7, batch:    120] loss: 0.05177 time model: 0.21768 acc: 0.68594
[epoch: 7, batch:    140] loss: 0.05153 time model: 0.21724 acc: 0.68571
[epoch: 7, batch:    160] loss: 0.05227 time model: 0.21694 acc: 0.68711
[epoch: 7, batch:    180] loss: 0.05259 time model: 0.21665 acc: 0.68194
[epoch: 7, batch:    200] loss: 0.05266 time model: 0.21643 acc: 0.68094
[epoch: 7, batch:    220] loss: 0.05212 time model: 0.21623 acc: 0.68295
[epoch: 7, batch:    240] loss: 0.05230 time model: 0.21606 acc: 0.68099
[epoch: 7, batch:    260] loss: 0.05186 time model: 0.21594 acc: 0.68389
[epoch: 7, batch:    280] loss: 0.05187 time model: 0.21586 acc: 0.68192
[epoch: 7, batch:    300] loss: 0.05163 time model: 0.21580 acc: 0.68417
[epoch: 7, batch:    320] loss: 0.05155 time model: 0.21575 acc: 0.68379
[epoch: 7, batch:    340] loss: 0.05105 time model: 0.21569 acc: 0.68713
[epoch: 7, batch:    360] loss: 0.05086 time model: 0.21564 acc: 0.68767
[epoch: 7, batch:    380] loss: 0.05100 time model: 0.21560 acc: 0.68832
[epoch: 7, batch:    400] loss: 0.05101 time model: 0.21557 acc: 0.68906
[epoch: 7, batch:    420] loss: 0.05080 time model: 0.21551 acc: 0.69063
[epoch: 7, batch:    440] loss: 0.05090 time model: 0.21547 acc: 0.69134
[epoch: 7, batch:    460] loss: 0.05072 time model: 0.21543 acc: 0.69307
[epoch: 7, batch:    480] loss: 0.05062 time model: 0.21540 acc: 0.69349
[epoch: 7, batch:    500] loss: 0.05090 time model: 0.21540 acc: 0.69250
[epoch: 7, batch:    520] loss: 0.05114 time model: 0.21542 acc: 0.69111
[epoch: 7, batch:    540] loss: 0.05105 time model: 0.21544 acc: 0.69213
[epoch: 7, batch:    560] loss: 0.05095 time model: 0.21546 acc: 0.69286
[epoch: 7, batch:    580] loss: 0.05088 time model: 0.21548 acc: 0.69375
[epoch: 7, batch:    600] loss: 0.05068 time model: 0.21549 acc: 0.69531
[epoch: 7, batch:    620] loss: 0.05046 time model: 0.21550 acc: 0.69657
[epoch: 7, batch:    640] loss: 0.05066 time model: 0.21549 acc: 0.69521
[epoch: 7, batch:    660] loss: 0.05076 time model: 0.21547 acc: 0.69508
[epoch: 7, batch:    680] loss: 0.05089 time model: 0.21544 acc: 0.69513
[epoch: 7, batch:    700] loss: 0.05096 time model: 0.21538 acc: 0.69491
[epoch: 7, batch:    720] loss: 0.05078 time model: 0.21530 acc: 0.69601
[epoch: 7, batch:    740] loss: 0.05073 time model: 0.21523 acc: 0.69637
[epoch: 7, batch:    760] loss: 0.05067 time model: 0.21517 acc: 0.69679
[epoch: 7, batch:    780] loss: 0.05063 time model: 0.21511 acc: 0.69720
[epoch: 7, batch:    800] loss: 0.05073 time model: 0.21505 acc: 0.69656
[epoch: 7, batch:    820] loss: 0.05086 time model: 0.21501 acc: 0.69573
[epoch: 7, batch:    840] loss: 0.05080 time model: 0.21497 acc: 0.69613
[epoch: 7, batch:    860] loss: 0.05075 time model: 0.21492 acc: 0.69542
[epoch: 7, batch:    880] loss: 0.05068 time model: 0.21487 acc: 0.69588
[epoch: 7, batch:    900] loss: 0.05073 time model: 0.21483 acc: 0.69521
[epoch: 7, batch:    920] loss: 0.05076 time model: 0.21480 acc: 0.69531
[epoch: 7, batch:    940] loss: 0.05068 time model: 0.21476 acc: 0.69594
[epoch: 7, batch:    960] loss: 0.05054 time model: 0.21472 acc: 0.69642
[epoch: 7, batch:    980] loss: 0.05065 time model: 0.21469 acc: 0.69636
[epoch: 7, batch:   1000] loss: 0.05056 time model: 0.21465 acc: 0.69663
[epoch: 7, batch:   1020] loss: 0.05053 time model: 0.21462 acc: 0.69681
[epoch: 7, batch:   1040] loss: 0.05050 time model: 0.21460 acc: 0.69706
[epoch: 7, batch:   1060] loss: 0.05045 time model: 0.21460 acc: 0.69699
[epoch: 7, batch:   1080] loss: 0.05048 time model: 0.21462 acc: 0.69740
[epoch: 7, batch:   1100] loss: 0.05057 time model: 0.21464 acc: 0.69750
[epoch: 7, batch:   1120] loss: 0.05049 time model: 0.21466 acc: 0.69754
[epoch: 7, batch:   1140] loss: 0.05049 time model: 0.21467 acc: 0.69775
[epoch: 7, batch:   1160] loss: 0.05045 time model: 0.21469 acc: 0.69822
[epoch: 7, batch:   1180] loss: 0.05053 time model: 0.21471 acc: 0.69783
[epoch: 7, batch:   1200] loss: 0.05055 time model: 0.21472 acc: 0.69703
[epoch: 7, batch:   1220] loss: 0.05058 time model: 0.21473 acc: 0.69688
[epoch: 7, batch:   1240] loss: 0.05066 time model: 0.21474 acc: 0.69617
[epoch: 7, batch:   1260] loss: 0.05055 time model: 0.21476 acc: 0.69683
[epoch: 7, batch:   1280] loss: 0.05054 time model: 0.21476 acc: 0.69668
[epoch: 7, batch:   1300] loss: 0.05056 time model: 0.21477 acc: 0.69654
[epoch: 7, batch:   1320] loss: 0.05061 time model: 0.21479 acc: 0.69595
[epoch: 7, batch:     20] loss: 0.09072 time model: 0.21584 acc: 0.61250
[epoch: 7, batch:     40] loss: 0.08290 time model: 0.21273 acc: 0.61719
[epoch: 7, batch:     60] loss: 0.08310 time model: 0.21147 acc: 0.62187
[epoch: 7, batch:     80] loss: 0.08096 time model: 0.21076 acc: 0.63203
[epoch: 7, batch:    100] loss: 0.07876 time model: 0.21031 acc: 0.63562
[epoch: 7, batch:    120] loss: 0.07837 time model: 0.20999 acc: 0.62760
[epoch: 7, batch:    140] loss: 0.07947 time model: 0.20976 acc: 0.62679
[epoch: 7, batch:    160] loss: 0.08187 time model: 0.20959 acc: 0.61484
epoch:7 train loss: 0.050611327307903776 train acc: 0.6959488272921108 valid loss: 0.08196995478755151 valid acc: 0.616220411331005
[epoch: 8, batch:     20] loss: 0.04353 time model: 0.22942 acc: 0.72500
[epoch: 8, batch:     40] loss: 0.04691 time model: 0.22271 acc: 0.72344
[epoch: 8, batch:     60] loss: 0.04751 time model: 0.22033 acc: 0.71562
[epoch: 8, batch:     80] loss: 0.04642 time model: 0.21904 acc: 0.72109
[epoch: 8, batch:    100] loss: 0.04550 time model: 0.21832 acc: 0.72250
[epoch: 8, batch:    120] loss: 0.04537 time model: 0.21788 acc: 0.72240
[epoch: 8, batch:    140] loss: 0.04539 time model: 0.21758 acc: 0.72188
[epoch: 8, batch:    160] loss: 0.04565 time model: 0.21738 acc: 0.71680
[epoch: 8, batch:    180] loss: 0.04631 time model: 0.21718 acc: 0.71354
[epoch: 8, batch:    200] loss: 0.04679 time model: 0.21703 acc: 0.71031
[epoch: 8, batch:    220] loss: 0.04689 time model: 0.21691 acc: 0.70966
[epoch: 8, batch:    240] loss: 0.04743 time model: 0.21681 acc: 0.70703
[epoch: 8, batch:    260] loss: 0.04717 time model: 0.21672 acc: 0.70913
[epoch: 8, batch:    280] loss: 0.04736 time model: 0.21664 acc: 0.70804
[epoch: 8, batch:    300] loss: 0.04749 time model: 0.21657 acc: 0.70646
[epoch: 8, batch:    320] loss: 0.04754 time model: 0.21649 acc: 0.70566
[epoch: 8, batch:    340] loss: 0.04727 time model: 0.21642 acc: 0.70680
[epoch: 8, batch:    360] loss: 0.04742 time model: 0.21637 acc: 0.70608
[epoch: 8, batch:    380] loss: 0.04755 time model: 0.21634 acc: 0.70576
[epoch: 8, batch:    400] loss: 0.04770 time model: 0.21630 acc: 0.70469
[epoch: 8, batch:    420] loss: 0.04740 time model: 0.21627 acc: 0.70655
[epoch: 8, batch:    440] loss: 0.04750 time model: 0.21624 acc: 0.70668
[epoch: 8, batch:    460] loss: 0.04739 time model: 0.21622 acc: 0.70761
[epoch: 8, batch:    480] loss: 0.04734 time model: 0.21620 acc: 0.70781
[epoch: 8, batch:    500] loss: 0.04758 time model: 0.21618 acc: 0.70575
[epoch: 8, batch:    520] loss: 0.04782 time model: 0.21616 acc: 0.70565
[epoch: 8, batch:    540] loss: 0.04792 time model: 0.21614 acc: 0.70463
[epoch: 8, batch:    560] loss: 0.04795 time model: 0.21612 acc: 0.70458
[epoch: 8, batch:    580] loss: 0.04820 time model: 0.21611 acc: 0.70420
[epoch: 8, batch:    600] loss: 0.04791 time model: 0.21609 acc: 0.70594
[epoch: 8, batch:    620] loss: 0.04812 time model: 0.21608 acc: 0.70474
[epoch: 8, batch:    640] loss: 0.04818 time model: 0.21607 acc: 0.70527
[epoch: 8, batch:    660] loss: 0.04808 time model: 0.21606 acc: 0.70653
[epoch: 8, batch:    680] loss: 0.04804 time model: 0.21604 acc: 0.70708
[epoch: 8, batch:    700] loss: 0.04796 time model: 0.21603 acc: 0.70696
[epoch: 8, batch:    720] loss: 0.04803 time model: 0.21602 acc: 0.70642
[epoch: 8, batch:    740] loss: 0.04792 time model: 0.21601 acc: 0.70752
[epoch: 8, batch:    760] loss: 0.04787 time model: 0.21601 acc: 0.70781
[epoch: 8, batch:    780] loss: 0.04796 time model: 0.21600 acc: 0.70809
[epoch: 8, batch:    800] loss: 0.04793 time model: 0.21600 acc: 0.70820
[epoch: 8, batch:    820] loss: 0.04782 time model: 0.21599 acc: 0.70793
[epoch: 8, batch:    840] loss: 0.04770 time model: 0.21598 acc: 0.70848
[epoch: 8, batch:    860] loss: 0.04770 time model: 0.21597 acc: 0.70778
[epoch: 8, batch:    880] loss: 0.04774 time model: 0.21597 acc: 0.70817
[epoch: 8, batch:    900] loss: 0.04775 time model: 0.21596 acc: 0.70819
[epoch: 8, batch:    920] loss: 0.04781 time model: 0.21596 acc: 0.70842
[epoch: 8, batch:    940] loss: 0.04780 time model: 0.21595 acc: 0.70891
[epoch: 8, batch:    960] loss: 0.04787 time model: 0.21595 acc: 0.70879
[epoch: 8, batch:    980] loss: 0.04779 time model: 0.21595 acc: 0.70937
[epoch: 8, batch:   1000] loss: 0.04776 time model: 0.21595 acc: 0.70937
[epoch: 8, batch:   1020] loss: 0.04771 time model: 0.21595 acc: 0.70999
[epoch: 8, batch:   1040] loss: 0.04778 time model: 0.21595 acc: 0.70956
[epoch: 8, batch:   1060] loss: 0.04778 time model: 0.21594 acc: 0.70949
[epoch: 8, batch:   1080] loss: 0.04780 time model: 0.21594 acc: 0.70914
[epoch: 8, batch:   1100] loss: 0.04776 time model: 0.21594 acc: 0.70920
[epoch: 8, batch:   1120] loss: 0.04783 time model: 0.21594 acc: 0.70932
[epoch: 8, batch:   1140] loss: 0.04784 time model: 0.21593 acc: 0.70916
[epoch: 8, batch:   1160] loss: 0.04780 time model: 0.21593 acc: 0.70937
[epoch: 8, batch:   1180] loss: 0.04785 time model: 0.21593 acc: 0.70906
[epoch: 8, batch:   1200] loss: 0.04789 time model: 0.21593 acc: 0.70896
[epoch: 8, batch:   1220] loss: 0.04789 time model: 0.21591 acc: 0.70897
[epoch: 8, batch:   1240] loss: 0.04797 time model: 0.21589 acc: 0.70892
[epoch: 8, batch:   1260] loss: 0.04805 time model: 0.21585 acc: 0.70883
[epoch: 8, batch:   1280] loss: 0.04797 time model: 0.21580 acc: 0.70879
[epoch: 8, batch:   1300] loss: 0.04797 time model: 0.21575 acc: 0.70937
[epoch: 8, batch:   1320] loss: 0.04805 time model: 0.21570 acc: 0.70941
[epoch: 8, batch:     20] loss: 0.07879 time model: 0.21015 acc: 0.59688
[epoch: 8, batch:     40] loss: 0.07501 time model: 0.20830 acc: 0.61875
[epoch: 8, batch:     60] loss: 0.07419 time model: 0.20748 acc: 0.60833
[epoch: 8, batch:     80] loss: 0.07564 time model: 0.20708 acc: 0.60625
[epoch: 8, batch:    100] loss: 0.07539 time model: 0.20682 acc: 0.61000
[epoch: 8, batch:    120] loss: 0.07491 time model: 0.20685 acc: 0.60938
[epoch: 8, batch:    140] loss: 0.07462 time model: 0.20702 acc: 0.60670
[epoch: 8, batch:    160] loss: 0.07481 time model: 0.20714 acc: 0.60781
epoch:8 train loss: 0.04804652054793912 train acc: 0.7094053541814735 valid loss: 0.07487676912238536 valid acc: 0.6069072564998059
[epoch: 9, batch:     20] loss: 0.05085 time model: 0.22340 acc: 0.70312
[epoch: 9, batch:     40] loss: 0.04700 time model: 0.21958 acc: 0.72969
[epoch: 9, batch:     60] loss: 0.04506 time model: 0.21808 acc: 0.73125
[epoch: 9, batch:     80] loss: 0.04525 time model: 0.21741 acc: 0.72188
[epoch: 9, batch:    100] loss: 0.04524 time model: 0.21708 acc: 0.71688
[epoch: 9, batch:    120] loss: 0.04562 time model: 0.21687 acc: 0.71615
[epoch: 9, batch:    140] loss: 0.04615 time model: 0.21670 acc: 0.71652
[epoch: 9, batch:    160] loss: 0.04555 time model: 0.21659 acc: 0.72188
[epoch: 9, batch:    180] loss: 0.04575 time model: 0.21650 acc: 0.72083
[epoch: 9, batch:    200] loss: 0.04596 time model: 0.21642 acc: 0.71937
[epoch: 9, batch:    220] loss: 0.04584 time model: 0.21637 acc: 0.72045
[epoch: 9, batch:    240] loss: 0.04571 time model: 0.21631 acc: 0.72057
[epoch: 9, batch:    260] loss: 0.04579 time model: 0.21627 acc: 0.72043
[epoch: 9, batch:    280] loss: 0.04609 time model: 0.21624 acc: 0.72165
[epoch: 9, batch:    300] loss: 0.04580 time model: 0.21621 acc: 0.72271
[epoch: 9, batch:    320] loss: 0.04592 time model: 0.21619 acc: 0.72168
[epoch: 9, batch:    340] loss: 0.04556 time model: 0.21616 acc: 0.72463
[epoch: 9, batch:    360] loss: 0.04547 time model: 0.21614 acc: 0.72413
[epoch: 9, batch:    380] loss: 0.04548 time model: 0.21613 acc: 0.72401
[epoch: 9, batch:    400] loss: 0.04571 time model: 0.21612 acc: 0.72391
[epoch: 9, batch:    420] loss: 0.04578 time model: 0.21610 acc: 0.72321
[epoch: 9, batch:    440] loss: 0.04594 time model: 0.21610 acc: 0.72188
[epoch: 9, batch:    460] loss: 0.04603 time model: 0.21608 acc: 0.72065
[epoch: 9, batch:    480] loss: 0.04614 time model: 0.21607 acc: 0.72005
[epoch: 9, batch:    500] loss: 0.04609 time model: 0.21606 acc: 0.72088
[epoch: 9, batch:    520] loss: 0.04606 time model: 0.21604 acc: 0.72151
[epoch: 9, batch:    540] loss: 0.04614 time model: 0.21603 acc: 0.72153
[epoch: 9, batch:    560] loss: 0.04619 time model: 0.21603 acc: 0.72176
[epoch: 9, batch:    580] loss: 0.04625 time model: 0.21601 acc: 0.72058
[epoch: 9, batch:    600] loss: 0.04609 time model: 0.21601 acc: 0.72135
[epoch: 9, batch:    620] loss: 0.04601 time model: 0.21600 acc: 0.72157
[epoch: 9, batch:    640] loss: 0.04581 time model: 0.21598 acc: 0.72295
[epoch: 9, batch:    660] loss: 0.04581 time model: 0.21594 acc: 0.72358
[epoch: 9, batch:    680] loss: 0.04569 time model: 0.21591 acc: 0.72335
[epoch: 9, batch:    700] loss: 0.04582 time model: 0.21590 acc: 0.72250
[epoch: 9, batch:    720] loss: 0.04572 time model: 0.21590 acc: 0.72274
[epoch: 9, batch:    740] loss: 0.04578 time model: 0.21590 acc: 0.72221
[epoch: 9, batch:    760] loss: 0.04582 time model: 0.21590 acc: 0.72245
[epoch: 9, batch:    780] loss: 0.04591 time model: 0.21590 acc: 0.72163
[epoch: 9, batch:    800] loss: 0.04591 time model: 0.21590 acc: 0.72062
[epoch: 9, batch:    820] loss: 0.04594 time model: 0.21589 acc: 0.72058
[epoch: 9, batch:    840] loss: 0.04606 time model: 0.21589 acc: 0.71994
[epoch: 9, batch:    860] loss: 0.04612 time model: 0.21588 acc: 0.71955
[epoch: 9, batch:    880] loss: 0.04597 time model: 0.21588 acc: 0.72031
[epoch: 9, batch:    900] loss: 0.04603 time model: 0.21588 acc: 0.71993
[epoch: 9, batch:    920] loss: 0.04613 time model: 0.21587 acc: 0.71923
[epoch: 9, batch:    940] loss: 0.04612 time model: 0.21587 acc: 0.71908
[epoch: 9, batch:    960] loss: 0.04607 time model: 0.21586 acc: 0.71940
[epoch: 9, batch:    980] loss: 0.04596 time model: 0.21586 acc: 0.72003
[epoch: 9, batch:   1000] loss: 0.04589 time model: 0.21586 acc: 0.72044
[epoch: 9, batch:   1020] loss: 0.04601 time model: 0.21586 acc: 0.71998
[epoch: 9, batch:   1040] loss: 0.04595 time model: 0.21586 acc: 0.72019
[epoch: 9, batch:   1060] loss: 0.04595 time model: 0.21586 acc: 0.72011
[epoch: 9, batch:   1080] loss: 0.04590 time model: 0.21586 acc: 0.72066
[epoch: 9, batch:   1100] loss: 0.04590 time model: 0.21585 acc: 0.72062
[epoch: 9, batch:   1120] loss: 0.04595 time model: 0.21585 acc: 0.72020
[epoch: 9, batch:   1140] loss: 0.04597 time model: 0.21585 acc: 0.71946
[epoch: 9, batch:   1160] loss: 0.04605 time model: 0.21584 acc: 0.71907
[epoch: 9, batch:   1180] loss: 0.04598 time model: 0.21584 acc: 0.71933
[epoch: 9, batch:   1200] loss: 0.04592 time model: 0.21584 acc: 0.71990
[epoch: 9, batch:   1220] loss: 0.04592 time model: 0.21584 acc: 0.71957
[epoch: 9, batch:   1240] loss: 0.04592 time model: 0.21583 acc: 0.71925
[epoch: 9, batch:   1260] loss: 0.04596 time model: 0.21583 acc: 0.71895
[epoch: 9, batch:   1280] loss: 0.04598 time model: 0.21582 acc: 0.71924
[epoch: 9, batch:   1300] loss: 0.04593 time model: 0.21579 acc: 0.71971
[epoch: 9, batch:   1320] loss: 0.04599 time model: 0.21578 acc: 0.71959
[epoch: 9, batch:     20] loss: 0.06277 time model: 0.21065 acc: 0.65938
[epoch: 9, batch:     40] loss: 0.05942 time model: 0.20828 acc: 0.68281
[epoch: 9, batch:     60] loss: 0.06003 time model: 0.20725 acc: 0.68646
[epoch: 9, batch:     80] loss: 0.06109 time model: 0.20681 acc: 0.67891
[epoch: 9, batch:    100] loss: 0.06026 time model: 0.20657 acc: 0.68312
[epoch: 9, batch:    120] loss: 0.05932 time model: 0.20635 acc: 0.68490
[epoch: 9, batch:    140] loss: 0.05827 time model: 0.20623 acc: 0.68839
[epoch: 9, batch:    160] loss: 0.05951 time model: 0.20614 acc: 0.68008
epoch:9 train loss: 0.04599488691409824 train acc: 0.7195925136223643 valid loss: 0.06080847526410776 valid acc: 0.6790842064415987
[epoch: 10, batch:     20] loss: 0.03922 time model: 0.22697 acc: 0.77187
[epoch: 10, batch:     40] loss: 0.04058 time model: 0.22103 acc: 0.75938
[epoch: 10, batch:     60] loss: 0.03936 time model: 0.21901 acc: 0.76562
[epoch: 10, batch:     80] loss: 0.03948 time model: 0.21800 acc: 0.75234
[epoch: 10, batch:    100] loss: 0.04187 time model: 0.21742 acc: 0.74875
[epoch: 10, batch:    120] loss: 0.04283 time model: 0.21711 acc: 0.74427
[epoch: 10, batch:    140] loss: 0.04336 time model: 0.21694 acc: 0.73750
[epoch: 10, batch:    160] loss: 0.04372 time model: 0.21680 acc: 0.73438
[epoch: 10, batch:    180] loss: 0.04338 time model: 0.21668 acc: 0.73715
[epoch: 10, batch:    200] loss: 0.04294 time model: 0.21658 acc: 0.73719
[epoch: 10, batch:    220] loss: 0.04272 time model: 0.21649 acc: 0.73608
[epoch: 10, batch:    240] loss: 0.04304 time model: 0.21644 acc: 0.73750
[epoch: 10, batch:    260] loss: 0.04316 time model: 0.21638 acc: 0.73678
[epoch: 10, batch:    280] loss: 0.04301 time model: 0.21632 acc: 0.73795
[epoch: 10, batch:    300] loss: 0.04302 time model: 0.21628 acc: 0.73771
[epoch: 10, batch:    320] loss: 0.04325 time model: 0.21625 acc: 0.73457
[epoch: 10, batch:    340] loss: 0.04318 time model: 0.21622 acc: 0.73585
[epoch: 10, batch:    360] loss: 0.04318 time model: 0.21619 acc: 0.73490
[epoch: 10, batch:    380] loss: 0.04318 time model: 0.21616 acc: 0.73421
[epoch: 10, batch:    400] loss: 0.04310 time model: 0.21614 acc: 0.73547
[epoch: 10, batch:    420] loss: 0.04290 time model: 0.21611 acc: 0.73661
[epoch: 10, batch:    440] loss: 0.04315 time model: 0.21608 acc: 0.73523
[epoch: 10, batch:    460] loss: 0.04298 time model: 0.21607 acc: 0.73696
[epoch: 10, batch:    480] loss: 0.04289 time model: 0.21604 acc: 0.73659
[epoch: 10, batch:    500] loss: 0.04294 time model: 0.21603 acc: 0.73600
[epoch: 10, batch:    520] loss: 0.04304 time model: 0.21601 acc: 0.73498
[epoch: 10, batch:    540] loss: 0.04319 time model: 0.21600 acc: 0.73287
[epoch: 10, batch:    560] loss: 0.04321 time model: 0.21598 acc: 0.73326
[epoch: 10, batch:    580] loss: 0.04326 time model: 0.21597 acc: 0.73287
[epoch: 10, batch:    600] loss: 0.04320 time model: 0.21596 acc: 0.73302
[epoch: 10, batch:    620] loss: 0.04326 time model: 0.21595 acc: 0.73246
[epoch: 10, batch:    640] loss: 0.04341 time model: 0.21594 acc: 0.73057
[epoch: 10, batch:    660] loss: 0.04343 time model: 0.21593 acc: 0.73049
[epoch: 10, batch:    680] loss: 0.04334 time model: 0.21592 acc: 0.73061
[epoch: 10, batch:    700] loss: 0.04335 time model: 0.21592 acc: 0.73080
[epoch: 10, batch:    720] loss: 0.04341 time model: 0.21591 acc: 0.73134
[epoch: 10, batch:    740] loss: 0.04336 time model: 0.21590 acc: 0.73150
[epoch: 10, batch:    760] loss: 0.04341 time model: 0.21590 acc: 0.73150
[epoch: 10, batch:    780] loss: 0.04331 time model: 0.21589 acc: 0.73149
[epoch: 10, batch:    800] loss: 0.04359 time model: 0.21588 acc: 0.72961
[epoch: 10, batch:    820] loss: 0.04362 time model: 0.21587 acc: 0.72950
[epoch: 10, batch:    840] loss: 0.04361 time model: 0.21585 acc: 0.72946
[epoch: 10, batch:    860] loss: 0.04364 time model: 0.21582 acc: 0.72987
[epoch: 10, batch:    880] loss: 0.04374 time model: 0.21580 acc: 0.72919
[epoch: 10, batch:    900] loss: 0.04373 time model: 0.21578 acc: 0.72931
[epoch: 10, batch:    920] loss: 0.04373 time model: 0.21576 acc: 0.72982
[epoch: 10, batch:    940] loss: 0.04376 time model: 0.21574 acc: 0.72959
[epoch: 10, batch:    960] loss: 0.04378 time model: 0.21570 acc: 0.72936
[epoch: 10, batch:    980] loss: 0.04375 time model: 0.21567 acc: 0.72946
[epoch: 10, batch:   1000] loss: 0.04376 time model: 0.21563 acc: 0.72913
[epoch: 10, batch:   1020] loss: 0.04402 time model: 0.21560 acc: 0.72739
[epoch: 10, batch:   1040] loss: 0.04400 time model: 0.21557 acc: 0.72813
[epoch: 10, batch:   1060] loss: 0.04400 time model: 0.21556 acc: 0.72907
[epoch: 10, batch:   1080] loss: 0.04400 time model: 0.21556 acc: 0.72917
[epoch: 10, batch:   1100] loss: 0.04409 time model: 0.21555 acc: 0.72858
[epoch: 10, batch:   1120] loss: 0.04421 time model: 0.21629 acc: 0.72813
[epoch: 10, batch:   1140] loss: 0.04416 time model: 0.21625 acc: 0.72829
[epoch: 10, batch:   1160] loss: 0.04408 time model: 0.21621 acc: 0.72866
[epoch: 10, batch:   1180] loss: 0.04410 time model: 0.21618 acc: 0.72897
                                                                          