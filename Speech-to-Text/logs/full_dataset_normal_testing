[epoch: 1, batch:     21] loss: 0.07330 time model: 0.02104 acc: 0.17813
[epoch: 1, batch:     21] loss: 0.06991 time model: 0.02084 acc: 0.16562
[epoch: 1, batch:     41] loss: 0.06716 time model: 0.02072 acc: 0.20234
[epoch: 1, batch:     61] loss: 0.06505 time model: 0.02069 acc: 0.22031
[epoch: 1, batch:     20] loss: 0.06081 time model: 0.02065 acc: 0.30000
[epoch: 1, batch:     20] loss: 0.07520 time model: 0.04116 acc: 0.12500
[epoch: 1, batch:     40] loss: 0.07332 time model: 0.04105 acc: 0.13125
[epoch: 1, batch:     20] loss: 0.07296 time model: 0.04087 acc: 0.14688
[epoch: 1, batch:     40] loss: 0.07208 time model: 0.04081 acc: 0.16328
[epoch: 1, batch:     60] loss: 0.07169 time model: 0.04078 acc: 0.17240
[epoch: 1, batch:     80] loss: 0.07093 time model: 0.04077 acc: 0.17148
[epoch: 1, batch:    100] loss: 0.07123 time model: 0.04077 acc: 0.16812
[epoch: 1, batch:    120] loss: 0.07054 time model: 0.04076 acc: 0.17448
[epoch: 1, batch:    140] loss: 0.07050 time model: 0.04075 acc: 0.17411
[epoch: 1, batch:    160] loss: 0.06964 time model: 0.04074 acc: 0.18242
[epoch: 1, batch:    180] loss: 0.06961 time model: 0.04073 acc: 0.18038
[epoch: 1, batch:    200] loss: 0.06932 time model: 0.04072 acc: 0.18563
[epoch: 1, batch:    220] loss: 0.06885 time model: 0.04071 acc: 0.18949
[epoch: 1, batch:    240] loss: 0.06908 time model: 0.04071 acc: 0.18932
[epoch: 1, batch:    260] loss: 0.06927 time model: 0.04070 acc: 0.18786
[epoch: 1, batch:    280] loss: 0.06920 time model: 0.04070 acc: 0.18929
[epoch: 1, batch:    300] loss: 0.06914 time model: 0.04070 acc: 0.18948
[epoch: 1, batch:    320] loss: 0.06910 time model: 0.04070 acc: 0.19043
[epoch: 1, batch:    340] loss: 0.06883 time model: 0.04070 acc: 0.19513
[epoch: 1, batch:    360] loss: 0.06884 time model: 0.04070 acc: 0.19687
[epoch: 1, batch:    380] loss: 0.06895 time model: 0.04070 acc: 0.19465
[epoch: 1, batch:    400] loss: 0.06904 time model: 0.04070 acc: 0.19344
[epoch: 1, batch:    420] loss: 0.06899 time model: 0.04071 acc: 0.19449
[epoch: 1, batch:    440] loss: 0.06860 time model: 0.04071 acc: 0.19915
[epoch: 1, batch:    460] loss: 0.06800 time model: 0.04070 acc: 0.20754
[epoch: 1, batch:    480] loss: 0.06773 time model: 0.04070 acc: 0.21152
[epoch: 1, batch:    500] loss: 0.06749 time model: 0.04070 acc: 0.21438
[epoch: 1, batch:    520] loss: 0.06729 time model: 0.04070 acc: 0.21647
[epoch: 1, batch:    540] loss: 0.06693 time model: 0.04070 acc: 0.22066
[epoch: 1, batch:    560] loss: 0.06654 time model: 0.04070 acc: 0.22550
[epoch: 1, batch:    580] loss: 0.06616 time model: 0.04070 acc: 0.22936
[epoch: 1, batch:    600] loss: 0.06561 time model: 0.04070 acc: 0.23542
[epoch: 1, batch:    620] loss: 0.06540 time model: 0.04070 acc: 0.23775
[epoch: 1, batch:    640] loss: 0.06518 time model: 0.04070 acc: 0.24019
[epoch: 1, batch:    660] loss: 0.06472 time model: 0.04071 acc: 0.24667
[epoch: 1, batch:     20] loss: 0.11643 time model: 0.02629 acc: 0.05781
[epoch: 1, batch:     40] loss: 0.11647 time model: 0.02630 acc: 0.06172
[epoch: 1, batch:     60] loss: 0.11646 time model: 0.02631 acc: 0.07135
[epoch: 1, batch:     80] loss: 0.11551 time model: 0.02631 acc: 0.06992
epoch:1 train loss: 0.06472376731582458 train acc: 0.2466714048803601 valid loss: 0.11609728538948387 valid acc: 0.06984866123399301
